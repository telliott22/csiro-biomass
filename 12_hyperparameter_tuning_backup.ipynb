{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning & Model 4 Variations\n",
    "\n",
    "## Objectives\n",
    "\n",
    "1. **Optuna Hyperparameter Search**: Systematically optimize Models 1 & 4\n",
    "2. **Model 4 Variations**: Test alternative knowledge transfer approaches\n",
    "3. **Final Comparison**: Train winners for 30 epochs and select best model for Kaggle\n",
    "\n",
    "## The Problem\n",
    "\n",
    "- **Model 3 (Multimodal)** achieved R² = +0.7537 at 30 epochs BUT cannot be used for Kaggle (requires weather/NDVI/species data)\n",
    "- **Test set only has images** → We need image-only models\n",
    "- **Challenge**: How to capture multimodal knowledge in an image-only model?\n",
    "\n",
    "## Approach\n",
    "\n",
    "### Models to Optimize\n",
    "\n",
    "1. **Model 1**: Simple ResNet18 baseline (image only)\n",
    "2. **Model 4a**: Teacher-Student distillation (learns from multimodal teacher)\n",
    "3. **Model 4b**: Auxiliary pretraining (learns image→tabular mapping first)\n",
    "\n",
    "### Timeline\n",
    "\n",
    "- **Debug mode** (DEBUG_MODE=True): ~30 min (5 trials, 2 epochs)\n",
    "- **Full run** (DEBUG_MODE=False): ~6-7 hours (50 trials, 30 epochs)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Part 1: Setup & Data Preparation\n\n### Install Required Packages (Run this first!)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from PIL import Image\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from tqdm.auto import tqdm\n",
    "import copy\n",
    "\n",
    "import optuna\n",
    "from optuna.trial import TrialState\n",
    "from optuna.visualization import plot_optimization_history, plot_param_importances\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "\n",
    "print(\"✓ Imports complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### Import Libraries",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Install Optuna and visualization dependencies\nimport sys\n!{sys.executable} -m pip install optuna plotly kaleido --quiet\nprint(\"✓ Dependencies installed: optuna, plotly, kaleido\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================\n",
    "# CONFIGURATION\n",
    "# ========================\n",
    "\n",
    "DEBUG_MODE = True  # Set to False for full training\n",
    "\n",
    "if DEBUG_MODE:\n",
    "    print(\"⚠️  DEBUG MODE: Quick testing\")\n",
    "    N_TRIALS = 5           # Optuna trials per model\n",
    "    EPOCHS_QUICK = 2       # Epochs per trial\n",
    "    EPOCHS_PHASE1 = 3      # Auxiliary pretraining\n",
    "    EPOCHS_PHASE2 = 3      # Auxiliary fine-tuning\n",
    "    EPOCHS_FINAL = 5       # Final winner training\n",
    "    TIMEOUT = 600          # 10 min timeout\n",
    "else:\n",
    "    print(\"✓ FULL TRAINING MODE\")\n",
    "    N_TRIALS = 50          # Thorough search\n",
    "    EPOCHS_QUICK = 10      # Good trial evaluation\n",
    "    EPOCHS_PHASE1 = 15     # Learn image→tabular\n",
    "    EPOCHS_PHASE2 = 20     # Fine-tune for biomass\n",
    "    EPOCHS_FINAL = 30      # Full training\n",
    "    TIMEOUT = 7200         # 2 hour timeout\n",
    "\n",
    "# Shared config\n",
    "BATCH_SIZE_DEFAULT = 16\n",
    "TARGET_COLS = ['Dry_Green_g', 'Dry_Dead_g', 'Dry_Clover_g', 'GDM_g', 'Dry_Total_g']\n",
    "COMPETITION_WEIGHTS = torch.tensor([0.1, 0.1, 0.1, 0.2, 0.5])\n",
    "\n",
    "print(f\"\\nConfiguration:\")\n",
    "print(f\"  N_TRIALS: {N_TRIALS}\")\n",
    "print(f\"  EPOCHS_QUICK: {EPOCHS_QUICK}\")\n",
    "print(f\"  EPOCHS_FINAL: {EPOCHS_FINAL}\")\n",
    "print(f\"  TIMEOUT: {TIMEOUT}s ({TIMEOUT/60:.0f} min)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train_enriched = pd.read_csv('competition/train_enriched.csv')\n",
    "train_enriched['Sampling_Date'] = pd.to_datetime(train_enriched['Sampling_Date'])\n",
    "train_enriched['full_image_path'] = train_enriched['image_path'].apply(lambda x: f'competition/{x}')\n",
    "\n",
    "# Train/val split\n",
    "train_data, val_data = train_test_split(train_enriched, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Data loaded: {len(train_data)} train, {len(val_data)} val\")\n",
    "print(f\"Targets: {TARGET_COLS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate target normalization stats (CRITICAL FIX!)\n",
    "target_means = torch.tensor([train_data[col].mean() for col in TARGET_COLS], dtype=torch.float32)\n",
    "target_stds = torch.tensor([train_data[col].std() for col in TARGET_COLS], dtype=torch.float32)\n",
    "\n",
    "print(\"Target normalization statistics:\")\n",
    "for i, col in enumerate(TARGET_COLS):\n",
    "    print(f\"  {col:15s}: mean={target_means[i]:.2f}g, std={target_stds[i]:.2f}g\")\n",
    "\n",
    "print(\"\\n✓ Will normalize all targets to mean=0, std=1 during training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare tabular features for multimodal/auxiliary training\n",
    "weather_features = [\n",
    "    'rainfall_7d', 'rainfall_30d',\n",
    "    'temp_max_7d', 'temp_min_7d', 'temp_mean_7d', 'temp_mean_30d', 'temp_range_7d',\n",
    "    'et0_7d', 'et0_30d',\n",
    "    'water_balance_7d', 'water_balance_30d',\n",
    "    'days_since_rain', 'daylength', 'season'\n",
    "]\n",
    "\n",
    "# Scale continuous features\n",
    "continuous_features = weather_features + ['Pre_GSHH_NDVI', 'Height_Ave_cm']\n",
    "scaler = StandardScaler()\n",
    "train_data[continuous_features] = scaler.fit_transform(train_data[continuous_features])\n",
    "val_data[continuous_features] = scaler.transform(val_data[continuous_features])\n",
    "\n",
    "# Encode categorical\n",
    "le_state = LabelEncoder()\n",
    "le_species = LabelEncoder()\n",
    "train_data['State_encoded'] = le_state.fit_transform(train_data['State'])\n",
    "train_data['Species_encoded'] = le_species.fit_transform(train_data['Species'])\n",
    "val_data['State_encoded'] = le_state.transform(val_data['State'])\n",
    "val_data['Species_encoded'] = le_species.transform(val_data['Species'])\n",
    "\n",
    "num_states = len(le_state.classes_)\n",
    "num_species = len(le_species.classes_)\n",
    "\n",
    "print(\"✓ Tabular features prepared\")\n",
    "print(f\"  Weather features: {len(weather_features)}\")\n",
    "print(f\"  States: {num_states}\")\n",
    "print(f\"  Species: {num_species}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalizedDataset(Dataset):\n",
    "    \"\"\"Image-only dataset with normalized targets.\"\"\"\n",
    "    def __init__(self, dataframe, target_means, target_stds, augment=False):\n",
    "        self.df = dataframe.reset_index(drop=True)\n",
    "        self.target_means = target_means\n",
    "        self.target_stds = target_stds\n",
    "        \n",
    "        transform_list = [transforms.Resize((224, 224))]\n",
    "        \n",
    "        if augment:\n",
    "            transform_list.extend([\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.RandomVerticalFlip(),\n",
    "                transforms.RandomRotation(10),\n",
    "            ])\n",
    "        \n",
    "        transform_list.extend([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        \n",
    "        self.transform = transforms.Compose(transform_list)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img = Image.open(row['full_image_path']).convert('RGB')\n",
    "        img = self.transform(img)\n",
    "        \n",
    "        targets = torch.tensor(row[TARGET_COLS].values.astype('float32'), dtype=torch.float32)\n",
    "        targets_normalized = (targets - self.target_means) / self.target_stds\n",
    "        \n",
    "        return {\n",
    "            'image': img,\n",
    "            'targets': targets_normalized,\n",
    "            'targets_original': targets\n",
    "        }\n",
    "\n",
    "class MultimodalDataset(Dataset):\n",
    "    \"\"\"Multimodal dataset for auxiliary pretraining.\"\"\"\n",
    "    def __init__(self, dataframe, target_means, target_stds, augment=False):\n",
    "        self.df = dataframe.reset_index(drop=True)\n",
    "        self.target_means = target_means\n",
    "        self.target_stds = target_stds\n",
    "        \n",
    "        transform_list = [transforms.Resize((224, 224))]\n",
    "        \n",
    "        if augment:\n",
    "            transform_list.extend([\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.RandomVerticalFlip(),\n",
    "                transforms.RandomRotation(10),\n",
    "            ])\n",
    "        \n",
    "        transform_list.extend([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        \n",
    "        self.transform = transforms.Compose(transform_list)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        \n",
    "        # Image\n",
    "        img = Image.open(row['full_image_path']).convert('RGB')\n",
    "        img = self.transform(img)\n",
    "        \n",
    "        # Tabular features\n",
    "        weather = torch.tensor(row[weather_features].values.astype('float32'), dtype=torch.float32)\n",
    "        ndvi = torch.tensor(row['Pre_GSHH_NDVI'], dtype=torch.float32)\n",
    "        height = torch.tensor(row['Height_Ave_cm'], dtype=torch.float32)\n",
    "        state = torch.tensor(row['State_encoded'], dtype=torch.long)\n",
    "        species = torch.tensor(row['Species_encoded'], dtype=torch.long)\n",
    "        \n",
    "        # Targets\n",
    "        targets = torch.tensor(row[TARGET_COLS].values.astype('float32'), dtype=torch.float32)\n",
    "        targets_normalized = (targets - self.target_means) / self.target_stds\n",
    "        \n",
    "        return {\n",
    "            'image': img,\n",
    "            'weather': weather,\n",
    "            'ndvi': ndvi,\n",
    "            'height': height,\n",
    "            'state': state,\n",
    "            'species': species,\n",
    "            'targets': targets_normalized,\n",
    "            'targets_original': targets\n",
    "        }\n",
    "\n",
    "print(\"✓ Dataset classes defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleBaseline(nn.Module):\n",
    "    \"\"\"Flexible ResNet18 baseline for hyperparameter tuning.\"\"\"\n",
    "    def __init__(self, num_outputs=5, hidden_dim=256, dropout=0.2, num_fc_layers=2):\n",
    "        super().__init__()\n",
    "        self.resnet = models.resnet18(pretrained=True)\n",
    "        num_features = self.resnet.fc.in_features  # 512\n",
    "        \n",
    "        # Build flexible FC layers\n",
    "        layers = []\n",
    "        in_features = num_features\n",
    "        \n",
    "        for i in range(num_fc_layers - 1):\n",
    "            layers.extend([\n",
    "                nn.Linear(in_features, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout)\n",
    "            ])\n",
    "            in_features = hidden_dim\n",
    "        \n",
    "        # Final layer\n",
    "        layers.append(nn.Linear(in_features, num_outputs))\n",
    "        \n",
    "        self.resnet.fc = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.resnet(x)\n",
    "    \n",
    "    def get_features(self, x):\n",
    "        \"\"\"Extract features before final FC (for distillation).\"\"\"\n",
    "        x = self.resnet.conv1(x)\n",
    "        x = self.resnet.bn1(x)\n",
    "        x = self.resnet.relu(x)\n",
    "        x = self.resnet.maxpool(x)\n",
    "        x = self.resnet.layer1(x)\n",
    "        x = self.resnet.layer2(x)\n",
    "        x = self.resnet.layer3(x)\n",
    "        x = self.resnet.layer4(x)\n",
    "        x = self.resnet.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        return x\n",
    "\n",
    "print(\"✓ SimpleBaseline defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalModel(nn.Module):\n",
    "    \"\"\"Multimodal model for teacher in distillation.\"\"\"\n",
    "    def __init__(self, num_outputs=5, num_states=4, num_species=15):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Image branch\n",
    "        self.resnet = models.resnet18(pretrained=True)\n",
    "        self.resnet = nn.Sequential(*list(self.resnet.children())[:-1])\n",
    "        cnn_features = 512\n",
    "        \n",
    "        # Weather branch\n",
    "        self.weather_encoder = nn.Sequential(\n",
    "            nn.Linear(14, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        \n",
    "        # Categorical embeddings\n",
    "        self.state_emb = nn.Embedding(num_states, 8)\n",
    "        self.species_emb = nn.Embedding(num_species, 16)\n",
    "        \n",
    "        # Tabular encoder\n",
    "        self.tabular_encoder = nn.Sequential(\n",
    "            nn.Linear(2 + 8 + 16, 32),  # ndvi/height + state + species\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        \n",
    "        # Fusion head\n",
    "        total_features = cnn_features + 64 + 32  # 608\n",
    "        self.fusion_head = nn.Sequential(\n",
    "            nn.Linear(total_features, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, num_outputs)\n",
    "        )\n",
    "    \n",
    "    def forward(self, images, weather, ndvi, height, state, species, return_features=False):\n",
    "        # Image features\n",
    "        img_feat = self.resnet(images)\n",
    "        img_feat = torch.flatten(img_feat, 1)\n",
    "        \n",
    "        # Weather features\n",
    "        weather_feat = self.weather_encoder(weather)\n",
    "        \n",
    "        # Tabular features\n",
    "        state_emb = self.state_emb(state)\n",
    "        species_emb = self.species_emb(species)\n",
    "        ndvi_height = torch.stack([ndvi, height], dim=1)\n",
    "        tabular_input = torch.cat([ndvi_height, state_emb, species_emb], dim=1)\n",
    "        tabular_feat = self.tabular_encoder(tabular_input)\n",
    "        \n",
    "        # Fuse\n",
    "        combined = torch.cat([img_feat, weather_feat, tabular_feat], dim=1)\n",
    "        output = self.fusion_head(combined)\n",
    "        \n",
    "        if return_features:\n",
    "            return output, img_feat\n",
    "        return output\n",
    "\n",
    "print(\"✓ MultimodalModel defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AuxiliaryPretrainedModel(nn.Module):\n",
    "    \"\"\"Model 4b: Two-phase training with auxiliary tasks.\n",
    "    \n",
    "    Phase 1: Train to predict tabular features from images\n",
    "    Phase 2: Fine-tune for biomass prediction\n",
    "    \n",
    "    At inference: Only needs image (learned implicit tabular patterns)\n",
    "    \"\"\"\n",
    "    def __init__(self, num_outputs=5, hidden_dim=256, dropout=0.2):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Shared backbone: ResNet18\n",
    "        self.backbone = models.resnet18(pretrained=True)\n",
    "        self.backbone = nn.Sequential(*list(self.backbone.children())[:-1])\n",
    "        \n",
    "        # Phase 1: Auxiliary heads (predict tabular features from image)\n",
    "        self.ndvi_head = nn.Linear(512, 1)           # Predict NDVI\n",
    "        self.height_head = nn.Linear(512, 1)         # Predict height\n",
    "        self.weather_head = nn.Linear(512, 14)       # Predict 14 weather features\n",
    "        self.state_head = nn.Linear(512, num_states)     # Predict state (classification)\n",
    "        self.species_head = nn.Linear(512, num_species)  # Predict species (classification)\n",
    "        \n",
    "        # Phase 2: Biomass prediction head (used after pretraining)\n",
    "        self.biomass_head = nn.Sequential(\n",
    "            nn.Linear(512, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, num_outputs)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, mode='biomass'):\n",
    "        features = self.backbone(x)\n",
    "        features = features.flatten(1)\n",
    "        \n",
    "        if mode == 'auxiliary':\n",
    "            # Phase 1: Predict tabular features\n",
    "            return {\n",
    "                'ndvi': self.ndvi_head(features),\n",
    "                'height': self.height_head(features),\n",
    "                'weather': self.weather_head(features),\n",
    "                'state': self.state_head(features),\n",
    "                'species': self.species_head(features)\n",
    "            }\n",
    "        else:  # mode == 'biomass'\n",
    "            # Phase 2: Predict biomass\n",
    "            return self.biomass_head(features)\n",
    "\n",
    "print(\"✓ AuxiliaryPretrainedModel defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_competition_r2(predictions, targets):\n",
    "    \"\"\"Calculate competition R² (weighted).\"\"\"\n",
    "    per_target_r2 = []\n",
    "    competition_r2 = 0\n",
    "    \n",
    "    weights = COMPETITION_WEIGHTS.numpy()\n",
    "    \n",
    "    for i in range(5):\n",
    "        r2 = r2_score(targets[:, i], predictions[:, i])\n",
    "        per_target_r2.append(r2)\n",
    "        competition_r2 += weights[i] * r2\n",
    "    \n",
    "    return competition_r2, per_target_r2\n",
    "\n",
    "class DistillationLoss(nn.Module):\n",
    "    \"\"\"Knowledge distillation loss for Model 4a.\"\"\"\n",
    "    def __init__(self, temperature=4.0, alpha=0.3, beta=0.5, gamma=0.2):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.gamma = gamma\n",
    "    \n",
    "    def forward(self, student_outputs, student_features, teacher_outputs, teacher_features, targets):\n",
    "        # Hard loss\n",
    "        hard_loss = F.mse_loss(student_outputs, targets)\n",
    "        \n",
    "        # Soft loss\n",
    "        soft_loss = F.mse_loss(student_outputs / self.temperature, teacher_outputs / self.temperature)\n",
    "        \n",
    "        # Feature loss\n",
    "        student_feat_norm = F.normalize(student_features, p=2, dim=1)\n",
    "        teacher_feat_norm = F.normalize(teacher_features, p=2, dim=1)\n",
    "        feature_loss = 1 - (student_feat_norm * teacher_feat_norm).sum(dim=1).mean()\n",
    "        \n",
    "        total_loss = self.alpha * hard_loss + self.beta * soft_loss + self.gamma * feature_loss\n",
    "        return total_loss\n",
    "\n",
    "print(\"✓ Utility functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: Optuna Hyperparameter Search\n",
    "\n",
    "We'll optimize:\n",
    "- Learning rate\n",
    "- Weight decay\n",
    "- Dropout\n",
    "- Batch size\n",
    "- Hidden dimensions\n",
    "- Scheduler type\n",
    "- Architecture depth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1: Simple Baseline Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_quick(model, train_loader, val_loader, num_epochs, lr, weight_decay, scheduler_type='ReduceLROnPlateau'):\n",
    "    \"\"\"Quick training for Optuna trials.\"\"\"\n",
    "    model = model.to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    \n",
    "    # Scheduler\n",
    "    if scheduler_type == 'ReduceLROnPlateau':\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)\n",
    "    elif scheduler_type == 'CosineAnnealing':\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "    else:  # StepLR\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=num_epochs//3, gamma=0.1)\n",
    "    \n",
    "    best_r2 = -float('inf')\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Train\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            images = batch['image'].to(device)\n",
    "            targets = batch['targets'].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item() * images.size(0)\n",
    "        \n",
    "        train_loss /= len(train_loader.dataset)\n",
    "        \n",
    "        # Validate\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        all_preds = []\n",
    "        all_targets = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                images = batch['image'].to(device)\n",
    "                targets = batch['targets'].to(device)\n",
    "                targets_original = batch['targets_original']\n",
    "                \n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, targets)\n",
    "                val_loss += loss.item() * images.size(0)\n",
    "                \n",
    "                # Denormalize for R² calculation\n",
    "                outputs_denorm = outputs.cpu() * target_stds + target_means\n",
    "                all_preds.append(outputs_denorm.numpy())\n",
    "                all_targets.append(targets_original.numpy())\n",
    "        \n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        \n",
    "        # Calculate R²\n",
    "        all_preds = np.vstack(all_preds)\n",
    "        all_targets = np.vstack(all_targets)\n",
    "        val_r2, _ = calculate_competition_r2(all_preds, all_targets)\n",
    "        \n",
    "        if scheduler_type == 'ReduceLROnPlateau':\n",
    "            scheduler.step(val_loss)\n",
    "        else:\n",
    "            scheduler.step()\n",
    "        \n",
    "        best_r2 = max(best_r2, val_r2)\n",
    "    \n",
    "    return best_r2\n",
    "\n",
    "print(\"✓ train_model_quick() defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_model1(trial):\n",
    "    \"\"\"Optuna objective for Model 1.\"\"\"\n",
    "    \n",
    "    # Hyperparameters to tune\n",
    "    lr = trial.suggest_float('lr', 1e-5, 1e-3, log=True)\n",
    "    weight_decay = trial.suggest_float('weight_decay', 1e-6, 1e-3, log=True)\n",
    "    dropout = trial.suggest_float('dropout', 0.1, 0.5)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [8, 16, 32])\n",
    "    hidden_dim = trial.suggest_categorical('hidden_dim', [128, 256, 512])\n",
    "    num_fc_layers = trial.suggest_int('num_fc_layers', 1, 3)\n",
    "    scheduler_type = trial.suggest_categorical('scheduler', ['ReduceLROnPlateau', 'CosineAnnealing', 'StepLR'])\n",
    "    \n",
    "    # Create datasets with trial batch_size\n",
    "    train_dataset = NormalizedDataset(train_data, target_means, target_stds, augment=True)\n",
    "    val_dataset = NormalizedDataset(val_data, target_means, target_stds, augment=False)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "    \n",
    "    # Build model\n",
    "    model = SimpleBaseline(\n",
    "        num_outputs=5,\n",
    "        hidden_dim=hidden_dim,\n",
    "        dropout=dropout,\n",
    "        num_fc_layers=num_fc_layers\n",
    "    )\n",
    "    \n",
    "    # Train and return validation R²\n",
    "    try:\n",
    "        val_r2 = train_model_quick(model, train_loader, val_loader, EPOCHS_QUICK, lr, weight_decay, scheduler_type)\n",
    "        return val_r2\n",
    "    except Exception as e:\n",
    "        print(f\"Trial failed: {e}\")\n",
    "        return -999.0  # Return very bad score on failure\n",
    "\n",
    "print(\"✓ objective_model1() defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Optuna study for Model 1\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"OPTUNA HYPERPARAMETER SEARCH: MODEL 1 (SIMPLE BASELINE)\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "study1 = optuna.create_study(\n",
    "    direction='maximize',\n",
    "    study_name='model1_hyperparam',\n",
    "    sampler=optuna.samplers.TPESampler(seed=42)\n",
    ")\n",
    "\n",
    "study1.optimize(objective_model1, n_trials=N_TRIALS, timeout=TIMEOUT, show_progress_bar=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"BEST TRIAL: R² = {study1.best_trial.value:+.4f}\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nBest hyperparameters:\")\n",
    "for key, value in study1.best_params.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Save study\n",
    "import pickle\n",
    "with open('optuna_study_model1.pkl', 'wb') as f:\n",
    "    pickle.dump(study1, f)\n",
    "print(\"\\n✓ Study saved to: optuna_study_model1.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Model 1 optimization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Optimization history\n",
    "ax = axes[0]\n",
    "trials = study1.trials\n",
    "trial_numbers = [t.number for t in trials if t.state == TrialState.COMPLETE]\n",
    "trial_values = [t.value for t in trials if t.state == TrialState.COMPLETE]\n",
    "ax.plot(trial_numbers, trial_values, 'o-', alpha=0.6)\n",
    "ax.axhline(y=study1.best_value, color='r', linestyle='--', label=f'Best: {study1.best_value:+.4f}')\n",
    "ax.set_xlabel('Trial', fontsize=12)\n",
    "ax.set_ylabel('Validation R²', fontsize=12)\n",
    "ax.set_title('Model 1: Optimization History', fontsize=14, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "# Parameter importance\n",
    "ax = axes[1]\n",
    "importances = optuna.importance.get_param_importances(study1)\n",
    "params = list(importances.keys())\n",
    "values = list(importances.values())\n",
    "ax.barh(params, values)\n",
    "ax.set_xlabel('Importance', fontsize=12)\n",
    "ax.set_title('Model 1: Parameter Importance', fontsize=14, fontweight='bold')\n",
    "ax.grid(alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('optuna_model1_analysis.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Visualization saved to: optuna_model1_analysis.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 4a: Teacher-Student Distillation Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we need a trained teacher (multimodal model)\n",
    "# Load the best teacher from previous training\n",
    "\n",
    "print(\"Loading pretrained teacher (Model 3 - Multimodal)...\")\n",
    "\n",
    "teacher = MultimodalModel(num_outputs=5, num_states=num_states, num_species=num_species)\n",
    "\n",
    "# Try to load best teacher weights\n",
    "import os\n",
    "if os.path.exists('Model_4_Teacher_best.pth'):\n",
    "    teacher.load_state_dict(torch.load('Model_4_Teacher_best.pth'))\n",
    "    print(\"✓ Loaded existing teacher weights\")\n",
    "elif os.path.exists('Model_3_Multimodal_best.pth'):\n",
    "    teacher.load_state_dict(torch.load('Model_3_Multimodal_best.pth'))\n",
    "    print(\"✓ Loaded existing multimodal weights\")\n",
    "else:\n",
    "    print(\"⚠️  No pretrained teacher found, will train one quickly...\")\n",
    "    # Quick teacher training\n",
    "    teacher = teacher.to(device)\n",
    "    teacher_dataset_train = MultimodalDataset(train_data, target_means, target_stds, augment=True)\n",
    "    teacher_dataset_val = MultimodalDataset(val_data, target_means, target_stds, augment=False)\n",
    "    teacher_loader_train = DataLoader(teacher_dataset_train, batch_size=16, shuffle=True, num_workers=0)\n",
    "    teacher_loader_val = DataLoader(teacher_dataset_val, batch_size=16, shuffle=False, num_workers=0)\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(teacher.parameters(), lr=3e-4, weight_decay=1e-4)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    for epoch in range(EPOCHS_QUICK):\n",
    "        teacher.train()\n",
    "        for batch in tqdm(teacher_loader_train, desc=f'Training Teacher Epoch {epoch+1}', leave=False):\n",
    "            images = batch['image'].to(device)\n",
    "            weather = batch['weather'].to(device)\n",
    "            ndvi = batch['ndvi'].to(device)\n",
    "            height = batch['height'].to(device)\n",
    "            state = batch['state'].to(device)\n",
    "            species = batch['species'].to(device)\n",
    "            targets = batch['targets'].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = teacher(images, weather, ndvi, height, state, species)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(teacher.parameters(), max_norm=10.0)\n",
    "            optimizer.step()\n",
    "    \n",
    "    torch.save(teacher.state_dict(), 'teacher_for_optuna.pth')\n",
    "    print(\"✓ Teacher trained and saved\")\n",
    "\n",
    "teacher.eval()\n",
    "teacher = teacher.to(device)\n",
    "print(\"✓ Teacher ready for distillation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_student_quick(student, teacher, train_loader_simple, train_loader_multi, val_loader, \n",
    "                       num_epochs, lr, weight_decay, temperature, alpha, beta, gamma):\n",
    "    \"\"\"Quick student training with distillation.\"\"\"\n",
    "    student = student.to(device)\n",
    "    teacher.eval()  # Freeze teacher\n",
    "    \n",
    "    # Normalize distillation weights\n",
    "    total = alpha + beta + gamma\n",
    "    alpha, beta, gamma = alpha/total, beta/total, gamma/total\n",
    "    \n",
    "    criterion = DistillationLoss(temperature, alpha, beta, gamma)\n",
    "    optimizer = torch.optim.AdamW(student.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    \n",
    "    best_r2 = -float('inf')\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Train\n",
    "        student.train()\n",
    "        \n",
    "        for batch_simple, batch_multi in zip(train_loader_simple, train_loader_multi):\n",
    "            # Student (image only)\n",
    "            images = batch_simple['image'].to(device)\n",
    "            targets = batch_simple['targets'].to(device)\n",
    "            \n",
    "            student_outputs = student(images)\n",
    "            student_features = student.get_features(images)\n",
    "            \n",
    "            # Teacher (multimodal)\n",
    "            with torch.no_grad():\n",
    "                teacher_images = batch_multi['image'].to(device)\n",
    "                teacher_weather = batch_multi['weather'].to(device)\n",
    "                teacher_ndvi = batch_multi['ndvi'].to(device)\n",
    "                teacher_height = batch_multi['height'].to(device)\n",
    "                teacher_state = batch_multi['state'].to(device)\n",
    "                teacher_species = batch_multi['species'].to(device)\n",
    "                \n",
    "                teacher_outputs, teacher_features = teacher(\n",
    "                    teacher_images, teacher_weather, teacher_ndvi, teacher_height,\n",
    "                    teacher_state, teacher_species, return_features=True\n",
    "                )\n",
    "            \n",
    "            # Distillation loss\n",
    "            loss = criterion(student_outputs, student_features, teacher_outputs, teacher_features, targets)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(student.parameters(), max_norm=10.0)\n",
    "            optimizer.step()\n",
    "        \n",
    "        # Validate\n",
    "        student.eval()\n",
    "        all_preds = []\n",
    "        all_targets = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                images = batch['image'].to(device)\n",
    "                targets_original = batch['targets_original']\n",
    "                \n",
    "                outputs = student(images)\n",
    "                \n",
    "                # Denormalize\n",
    "                outputs_denorm = outputs.cpu() * target_stds + target_means\n",
    "                all_preds.append(outputs_denorm.numpy())\n",
    "                all_targets.append(targets_original.numpy())\n",
    "        \n",
    "        all_preds = np.vstack(all_preds)\n",
    "        all_targets = np.vstack(all_targets)\n",
    "        val_r2, _ = calculate_competition_r2(all_preds, all_targets)\n",
    "        \n",
    "        best_r2 = max(best_r2, val_r2)\n",
    "    \n",
    "    return best_r2\n",
    "\n",
    "print(\"✓ train_student_quick() defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_model4(trial):\n",
    "    \"\"\"Optuna objective for Model 4a (Teacher-Student).\"\"\"\n",
    "    \n",
    "    # Student hyperparameters\n",
    "    lr = trial.suggest_float('lr', 1e-5, 1e-3, log=True)\n",
    "    weight_decay = trial.suggest_float('weight_decay', 1e-6, 1e-3, log=True)\n",
    "    dropout = trial.suggest_float('dropout', 0.1, 0.5)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [8, 16, 32])\n",
    "    hidden_dim = trial.suggest_categorical('hidden_dim', [128, 256, 512])\n",
    "    \n",
    "    # Distillation hyperparameters\n",
    "    temperature = trial.suggest_float('temperature', 2.0, 8.0)\n",
    "    alpha = trial.suggest_float('alpha', 0.1, 0.5)  # Hard loss weight\n",
    "    beta = trial.suggest_float('beta', 0.3, 0.7)    # Soft loss weight\n",
    "    gamma = trial.suggest_float('gamma', 0.1, 0.3)  # Feature loss weight\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset_simple = NormalizedDataset(train_data, target_means, target_stds, augment=True)\n",
    "    train_dataset_multi = MultimodalDataset(train_data, target_means, target_stds, augment=True)\n",
    "    val_dataset = NormalizedDataset(val_data, target_means, target_stds, augment=False)\n",
    "    \n",
    "    train_loader_simple = DataLoader(train_dataset_simple, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    train_loader_multi = DataLoader(train_dataset_multi, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "    \n",
    "    # Create student\n",
    "    student = SimpleBaseline(\n",
    "        num_outputs=5,\n",
    "        hidden_dim=hidden_dim,\n",
    "        dropout=dropout,\n",
    "        num_fc_layers=2\n",
    "    )\n",
    "    \n",
    "    # Train with distillation\n",
    "    try:\n",
    "        val_r2 = train_student_quick(\n",
    "            student, teacher, train_loader_simple, train_loader_multi, val_loader,\n",
    "            EPOCHS_QUICK, lr, weight_decay, temperature, alpha, beta, gamma\n",
    "        )\n",
    "        return val_r2\n",
    "    except Exception as e:\n",
    "        print(f\"Trial failed: {e}\")\n",
    "        return -999.0\n",
    "\n",
    "print(\"✓ objective_model4() defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Optuna study for Model 4a\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"OPTUNA HYPERPARAMETER SEARCH: MODEL 4A (TEACHER-STUDENT)\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "study4 = optuna.create_study(\n",
    "    direction='maximize',\n",
    "    study_name='model4_distillation',\n",
    "    sampler=optuna.samplers.TPESampler(seed=42)\n",
    ")\n",
    "\n",
    "study4.optimize(objective_model4, n_trials=N_TRIALS, timeout=TIMEOUT, show_progress_bar=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"BEST TRIAL: R² = {study4.best_trial.value:+.4f}\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nBest hyperparameters:\")\n",
    "for key, value in study4.best_params.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Save study\n",
    "with open('optuna_study_model4.pkl', 'wb') as f:\n",
    "    pickle.dump(study4, f)\n",
    "print(\"\\n✓ Study saved to: optuna_study_model4.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Model 4a optimization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Optimization history\n",
    "ax = axes[0]\n",
    "trials = study4.trials\n",
    "trial_numbers = [t.number for t in trials if t.state == TrialState.COMPLETE]\n",
    "trial_values = [t.value for t in trials if t.state == TrialState.COMPLETE]\n",
    "ax.plot(trial_numbers, trial_values, 'o-', alpha=0.6)\n",
    "ax.axhline(y=study4.best_value, color='r', linestyle='--', label=f'Best: {study4.best_value:+.4f}')\n",
    "ax.set_xlabel('Trial', fontsize=12)\n",
    "ax.set_ylabel('Validation R²', fontsize=12)\n",
    "ax.set_title('Model 4a: Optimization History', fontsize=14, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "# Parameter importance\n",
    "ax = axes[1]\n",
    "importances = optuna.importance.get_param_importances(study4)\n",
    "params = list(importances.keys())\n",
    "values = list(importances.values())\n",
    "ax.barh(params, values)\n",
    "ax.set_xlabel('Importance', fontsize=12)\n",
    "ax.set_title('Model 4a: Parameter Importance', fontsize=14, fontweight='bold')\n",
    "ax.grid(alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('optuna_model4_analysis.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Visualization saved to: optuna_model4_analysis.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optuna Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"OPTUNA HYPERPARAMETER SEARCH SUMMARY\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "summary_df = pd.DataFrame({\n",
    "    'Model': ['1. Simple Baseline', '4a. Teacher-Student'],\n",
    "    'Best R²': [f\"{study1.best_value:+.4f}\", f\"{study4.best_value:+.4f}\"],\n",
    "    'Trials Completed': [len([t for t in study1.trials if t.state == TrialState.COMPLETE]),\n",
    "                         len([t for t in study4.trials if t.state == TrialState.COMPLETE])],\n",
    "    'Best LR': [f\"{study1.best_params['lr']:.2e}\", f\"{study4.best_params['lr']:.2e}\"],\n",
    "    'Best Dropout': [f\"{study1.best_params['dropout']:.2f}\", f\"{study4.best_params['dropout']:.2f}\"]\n",
    "})\n",
    "\n",
    "print(summary_df.to_string(index=False))\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update todo list\n",
    "from IPython.display import Markdown\n",
    "Markdown(\"\"\"\n",
    "## ✓ Section 2 Complete: Optuna Hyperparameter Search\n",
    "\n",
    "**Completed:**\n",
    "- Model 1 optimization ({} trials)\n",
    "- Model 4a optimization ({} trials)\n",
    "- Visualization and analysis\n",
    "\n",
    "**Next:** Model 4b - Auxiliary Pretraining\n",
    "\"\"\".format(len(study1.trials), len(study4.trials)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3: Model 4b - Auxiliary Pretraining\n",
    "\n",
    "**Hypothesis**: If we train the model to predict tabular features (NDVI, height, weather, species) from images first, it will learn visual patterns that correlate with those features. Then when we fine-tune for biomass, the model can leverage this implicit understanding.\n",
    "\n",
    "**Two-Phase Training:**\n",
    "1. **Phase 1**: Train image → tabular (NDVI, height, weather, state, species)\n",
    "2. **Phase 2**: Fine-tune for biomass prediction\n",
    "\n",
    "**At inference**: Only needs image (learned to \"see\" tabular patterns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase 1: Auxiliary Pretraining (Image → Tabular)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auxiliary_loss(predictions, targets):\n",
    "    \"\"\"Multi-task loss for predicting tabular features.\"\"\"\n",
    "    loss_ndvi = F.mse_loss(predictions['ndvi'].squeeze(), targets['ndvi'])\n",
    "    loss_height = F.mse_loss(predictions['height'].squeeze(), targets['height'])\n",
    "    loss_weather = F.mse_loss(predictions['weather'], targets['weather'])\n",
    "    loss_state = F.cross_entropy(predictions['state'], targets['state'])\n",
    "    loss_species = F.cross_entropy(predictions['species'], targets['species'])\n",
    "    \n",
    "    # Weighted combination\n",
    "    total_loss = (\n",
    "        0.2 * loss_ndvi + \n",
    "        0.2 * loss_height + \n",
    "        0.3 * loss_weather + \n",
    "        0.15 * loss_state + \n",
    "        0.15 * loss_species\n",
    "    )\n",
    "    \n",
    "    return total_loss, {\n",
    "        'ndvi': loss_ndvi.item(),\n",
    "        'height': loss_height.item(),\n",
    "        'weather': loss_weather.item(),\n",
    "        'state': loss_state.item(),\n",
    "        'species': loss_species.item()\n",
    "    }\n",
    "\n",
    "print(\"✓ auxiliary_loss() defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_auxiliary_phase1(model, train_loader, val_loader, num_epochs):\n",
    "    \"\"\"Phase 1: Train to predict tabular features from images.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"MODEL 4B - PHASE 1: AUXILIARY PRETRAINING (Image → Tabular)\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-4)\n",
    "    \n",
    "    history = {'train_loss': [], 'val_loss': [], 'val_ndvi_mae': [], 'val_state_acc': []}\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Train\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        \n",
    "        for batch in tqdm(train_loader, desc=f'Phase 1 Epoch {epoch+1}/{num_epochs}', leave=False):\n",
    "            images = batch['image'].to(device)\n",
    "            targets = {\n",
    "                'ndvi': batch['ndvi'].to(device),\n",
    "                'height': batch['height'].to(device),\n",
    "                'weather': batch['weather'].to(device),\n",
    "                'state': batch['state'].to(device),\n",
    "                'species': batch['species'].to(device)\n",
    "            }\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(images, mode='auxiliary')\n",
    "            loss, _ = auxiliary_loss(predictions, targets)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item() * images.size(0)\n",
    "        \n",
    "        train_loss /= len(train_loader.dataset)\n",
    "        \n",
    "        # Validate\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        ndvi_errors = []\n",
    "        state_correct = 0\n",
    "        state_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                images = batch['image'].to(device)\n",
    "                targets = {\n",
    "                    'ndvi': batch['ndvi'].to(device),\n",
    "                    'height': batch['height'].to(device),\n",
    "                    'weather': batch['weather'].to(device),\n",
    "                    'state': batch['state'].to(device),\n",
    "                    'species': batch['species'].to(device)\n",
    "                }\n",
    "                \n",
    "                predictions = model(images, mode='auxiliary')\n",
    "                loss, _ = auxiliary_loss(predictions, targets)\n",
    "                val_loss += loss.item() * images.size(0)\n",
    "                \n",
    "                # NDVI MAE\n",
    "                ndvi_errors.extend((predictions['ndvi'].squeeze().cpu() - targets['ndvi'].cpu()).abs().tolist())\n",
    "                \n",
    "                # State accuracy\n",
    "                state_pred = predictions['state'].argmax(dim=1)\n",
    "                state_correct += (state_pred == targets['state']).sum().item()\n",
    "                state_total += targets['state'].size(0)\n",
    "        \n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        ndvi_mae = np.mean(ndvi_errors)\n",
    "        state_acc = state_correct / state_total\n",
    "        \n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_ndvi_mae'].append(ndvi_mae)\n",
    "        history['val_state_acc'].append(state_acc)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1:2d}/{num_epochs}: Train Loss={train_loss:.4f}, Val Loss={val_loss:.4f}, \"\n",
    "              f\"NDVI MAE={ndvi_mae:.4f}, State Acc={state_acc:.2%}\")\n",
    "    \n",
    "    print(f\"\\n✓ Phase 1 complete! Model learned to predict tabular features from images.\\n\")\n",
    "    return history\n",
    "\n",
    "print(\"✓ train_auxiliary_phase1() defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Model 4b Phase 1\n",
    "model_4b = AuxiliaryPretrainedModel(num_outputs=5, hidden_dim=256, dropout=0.2)\n",
    "\n",
    "# Create multimodal dataloaders\n",
    "train_dataset_multi = MultimodalDataset(train_data, target_means, target_stds, augment=True)\n",
    "val_dataset_multi = MultimodalDataset(val_data, target_means, target_stds, augment=False)\n",
    "train_loader_multi = DataLoader(train_dataset_multi, batch_size=BATCH_SIZE_DEFAULT, shuffle=True, num_workers=0)\n",
    "val_loader_multi = DataLoader(val_dataset_multi, batch_size=BATCH_SIZE_DEFAULT, shuffle=False, num_workers=0)\n",
    "\n",
    "history_4b_phase1 = train_auxiliary_phase1(model_4b, train_loader_multi, val_loader_multi, EPOCHS_PHASE1)\n",
    "\n",
    "# Save Phase 1 weights\n",
    "torch.save(model_4b.state_dict(), 'model_4b_phase1.pth')\n",
    "print(\"✓ Phase 1 weights saved to: model_4b_phase1.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase 2: Fine-tune for Biomass Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_auxiliary_phase2(model, train_loader, val_loader, num_epochs, freeze_backbone=False):\n",
    "    \"\"\"Phase 2: Fine-tune for biomass prediction.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"MODEL 4B - PHASE 2: BIOMASS FINE-TUNING\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    model = model.to(device)\n",
    "    \n",
    "    if freeze_backbone:\n",
    "        print(\"⚠️  Freezing backbone (only train biomass head)\")\n",
    "        for param in model.backbone.parameters():\n",
    "            param.requires_grad = False\n",
    "        optimizer = torch.optim.AdamW(model.biomass_head.parameters(), lr=3e-4, weight_decay=1e-4)\n",
    "    else:\n",
    "        print(\"✓ Fine-tuning entire model (backbone with low LR)\")\n",
    "        optimizer = torch.optim.AdamW([\n",
    "            {'params': model.backbone.parameters(), 'lr': 1e-5},\n",
    "            {'params': model.biomass_head.parameters(), 'lr': 3e-4}\n",
    "        ], weight_decay=1e-4)\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    history = {'train_loss': [], 'val_loss': [], 'val_r2': [], 'epoch': []}\n",
    "    best_r2 = -float('inf')\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Train\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        \n",
    "        for batch in tqdm(train_loader, desc=f'Phase 2 Epoch {epoch+1}/{num_epochs}', leave=False):\n",
    "            images = batch['image'].to(device)\n",
    "            targets = batch['targets'].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images, mode='biomass')\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item() * images.size(0)\n",
    "        \n",
    "        train_loss /= len(train_loader.dataset)\n",
    "        \n",
    "        # Validate\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        all_preds = []\n",
    "        all_targets = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                images = batch['image'].to(device)\n",
    "                targets = batch['targets'].to(device)\n",
    "                targets_original = batch['targets_original']\n",
    "                \n",
    "                outputs = model(images, mode='biomass')\n",
    "                loss = criterion(outputs, targets)\n",
    "                val_loss += loss.item() * images.size(0)\n",
    "                \n",
    "                # Denormalize\n",
    "                outputs_denorm = outputs.cpu() * target_stds + target_means\n",
    "                all_preds.append(outputs_denorm.numpy())\n",
    "                all_targets.append(targets_original.numpy())\n",
    "        \n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        \n",
    "        # Calculate R²\n",
    "        all_preds = np.vstack(all_preds)\n",
    "        all_targets = np.vstack(all_targets)\n",
    "        val_r2, _ = calculate_competition_r2(all_preds, all_targets)\n",
    "        \n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_r2'].append(val_r2)\n",
    "        history['epoch'].append(epoch + 1)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1:2d}/{num_epochs}: Train Loss={train_loss:.4f}, Val Loss={val_loss:.4f}, Val R²={val_r2:+.4f}\")\n",
    "        \n",
    "        if val_r2 > best_r2:\n",
    "            best_r2 = val_r2\n",
    "            torch.save(model.state_dict(), 'model_4b_phase2_best.pth')\n",
    "            print(f\"  💾 New best R²={best_r2:+.4f}\")\n",
    "    \n",
    "    print(f\"\\n✓ Phase 2 complete! Best R²={best_r2:+.4f}\\n\")\n",
    "    return history, best_r2\n",
    "\n",
    "print(\"✓ train_auxiliary_phase2() defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Model 4b Phase 2\n",
    "# Create image-only dataloaders\n",
    "train_dataset_simple = NormalizedDataset(train_data, target_means, target_stds, augment=True)\n",
    "val_dataset_simple = NormalizedDataset(val_data, target_means, target_stds, augment=False)\n",
    "train_loader_simple = DataLoader(train_dataset_simple, batch_size=BATCH_SIZE_DEFAULT, shuffle=True, num_workers=0)\n",
    "val_loader_simple = DataLoader(val_dataset_simple, batch_size=BATCH_SIZE_DEFAULT, shuffle=False, num_workers=0)\n",
    "\n",
    "history_4b_phase2, best_r2_4b = train_auxiliary_phase2(\n",
    "    model_4b, train_loader_simple, val_loader_simple, EPOCHS_PHASE2, freeze_backbone=False\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Model 4b training complete!\")\n",
    "print(f\"  Phase 1: Learned image→tabular mapping\")\n",
    "print(f\"  Phase 2: Fine-tuned for biomass, R²={best_r2_4b:+.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Model 4 Variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL 4 VARIANTS COMPARISON\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "model4_comparison = pd.DataFrame({\n",
    "    'Model': ['4a. Teacher-Student', '4b. Auxiliary Pretrain'],\n",
    "    'Best R² (Optuna)': [f\"{study4.best_value:+.4f}\", f\"{best_r2_4b:+.4f}\"],\n",
    "    'Approach': ['Knowledge distillation', 'Multi-task pretraining'],\n",
    "    'Training': ['Student learns from teacher', 'Phase 1: Image→tabular, Phase 2: Biomass'],\n",
    "    'Advantage': ['Soft targets from teacher', 'Explicit tabular-visual alignment']\n",
    "})\n",
    "\n",
    "print(model4_comparison.to_string(index=False))\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "if best_r2_4b > study4.best_value:\n",
    "    print(f\"\\n🏆 Model 4b (Auxiliary) wins! R²={best_r2_4b:+.4f} vs {study4.best_value:+.4f}\")\n",
    "    print(\"   → Auxiliary pretraining successfully transfers tabular knowledge!\")\n",
    "    best_model4_variant = '4b'\n",
    "    best_model4_r2 = best_r2_4b\n",
    "else:\n",
    "    print(f\"\\n🏆 Model 4a (Distillation) wins! R²={study4.best_value:+.4f} vs {best_r2_4b:+.4f}\")\n",
    "    print(\"   → Traditional distillation is more effective for this task\")\n",
    "    best_model4_variant = '4a'\n",
    "    best_model4_r2 = study4.best_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Model 4b training\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Phase 1: Auxiliary training\n",
    "ax = axes[0]\n",
    "epochs_p1 = list(range(1, len(history_4b_phase1['train_loss']) + 1))\n",
    "ax.plot(epochs_p1, history_4b_phase1['train_loss'], 'o-', label='Train Loss', linewidth=2)\n",
    "ax.plot(epochs_p1, history_4b_phase1['val_loss'], 's-', label='Val Loss', linewidth=2)\n",
    "ax2 = ax.twinx()\n",
    "ax2.plot(epochs_p1, history_4b_phase1['val_ndvi_mae'], '^-', color='green', label='NDVI MAE', linewidth=2)\n",
    "ax.set_xlabel('Epoch', fontsize=12)\n",
    "ax.set_ylabel('Loss', fontsize=12)\n",
    "ax2.set_ylabel('NDVI MAE', fontsize=12, color='green')\n",
    "ax.set_title('Model 4b Phase 1: Image→Tabular Training', fontsize=14, fontweight='bold')\n",
    "ax.legend(loc='upper left')\n",
    "ax2.legend(loc='upper right')\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "# Phase 2: Biomass training\n",
    "ax = axes[1]\n",
    "ax.plot(history_4b_phase2['epoch'], history_4b_phase2['val_r2'], 'o-', linewidth=2)\n",
    "ax.axhline(y=best_r2_4b, color='r', linestyle='--', label=f'Best: {best_r2_4b:+.4f}')\n",
    "ax.set_xlabel('Epoch', fontsize=12)\n",
    "ax.set_ylabel('Validation R²', fontsize=12)\n",
    "ax.set_title('Model 4b Phase 2: Biomass Fine-tuning', fontsize=14, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('model_4b_training.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Visualization saved to: model_4b_training.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 4: Final Training & Comparison (30 Epochs)\n",
    "\n",
    "Now we'll train the optimized models for the full duration and compare results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This section will be populated based on your decision to proceed with 30-epoch training\n",
    "# For now, let's create a summary of what we have\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL SUMMARY: READY FOR 30-EPOCH TRAINING\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "final_summary = pd.DataFrame({\n",
    "    'Model': [\n",
    "        '1. Simple (Optimized)',\n",
    "        f'4{best_model4_variant}. Best Model 4',\n",
    "        '3. Multimodal (Reference)'\n",
    "    ],\n",
    "    f'R² ({EPOCHS_QUICK} epochs)': [\n",
    "        f\"{study1.best_value:+.4f}\",\n",
    "        f\"{best_model4_r2:+.4f}\",\n",
    "        'N/A'\n",
    "    ],\n",
    "    'R² (30 epochs)': [\n",
    "        'TODO: Train with best params',\n",
    "        'TODO: Train with best params',\n",
    "        '+0.7537 (from notebook 11)'\n",
    "    ],\n",
    "    'Can Submit?': ['✅ Yes', '✅ Yes', '❌ No (needs tabular)'],\n",
    "    'Hyperparameters': ['Optuna optimized', 'Optuna/default', 'Default']\n",
    "})\n",
    "\n",
    "print(final_summary.to_string(index=False))\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "print(\"\\n📋 Next Steps:\")\n",
    "print(\"  1. Set DEBUG_MODE=False at the top of notebook\")\n",
    "print(\"  2. Re-run Optuna searches (50 trials each, ~4 hours)\")\n",
    "print(\"  3. Train Model 1 with best params for 30 epochs (~1 hour)\")\n",
    "print(f\"  4. Train Model {best_model4_variant} with best params for 30 epochs (~1 hour)\")\n",
    "print(\"  5. Compare final results and select winner for Kaggle\")\n",
    "print(\"\\n✓ Notebook structure complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Notebook Complete!\n",
    "\n",
    "### What We've Built:\n",
    "\n",
    "1. **Optuna Hyperparameter Search** for Models 1 & 4a\n",
    "2. **Model 4b**: Auxiliary pretraining approach (image→tabular→biomass)\n",
    "3. **Comparison framework** to evaluate all approaches\n",
    "\n",
    "### Key Findings (Debug Mode):\n",
    "\n",
    "- Best Model 1 hyperparameters found\n",
    "- Best Model 4a distillation parameters found  \n",
    "- Model 4b auxiliary pretraining tested\n",
    "- All models are image-only and can be used for Kaggle submission\n",
    "\n",
    "### To Run Full Experiment:\n",
    "\n",
    "```python\n",
    "# Set at top of notebook:\n",
    "DEBUG_MODE = False\n",
    "```\n",
    "\n",
    "Then:\n",
    "- Run full Optuna search (50 trials, 10 epochs each)\n",
    "- Train winners for 30 epochs\n",
    "- Select best model for Kaggle test predictions\n",
    "\n",
    "### Expected Final Results:\n",
    "\n",
    "- **Model 1 (optimized)**: R² ~ 0.70-0.75\n",
    "- **Model 4 (best variant)**: R² ~ 0.65-0.72\n",
    "- **Winner**: Likely Model 1 (simplicity + optimization)\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}