{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Function Comparison Experiment\n",
    "\n",
    "## Critical Issues Identified\n",
    "\n",
    "From debugging ([09_debug_training.ipynb](09_debug_training.ipynb)):\n",
    "\n",
    "**üî¥ Issue #1: Output Scale Wrong**\n",
    "- Model predictions: **-0.5 to +0.5** (range: 1)\n",
    "- Actual targets: **0 to 200g** (range: 200)\n",
    "- **200√ó scale mismatch!**\n",
    "\n",
    "**üî¥ Issue #2: Cannot Overfit Single Batch**\n",
    "- After 100 training steps on 1 batch: **R¬≤ = -2.74**\n",
    "- Expected: **R¬≤ > 0.9** (model should memorize)\n",
    "- **Architecture is broken!**\n",
    "\n",
    "**üî¥ Issue #3: Competition Weights in Loss**\n",
    "- Dry_Total_g: 50% weight + largest values = **dominates training**\n",
    "- Other targets get ignored\n",
    "- **Competition weights are for EVALUATION, not TRAINING!**\n",
    "\n",
    "## Hypothesis\n",
    "\n",
    "**The problem: Using competition weights during training causes:**\n",
    "1. Model focuses only on Dry_Total_g (50% weight)\n",
    "2. Output scale mismatch (predicting ~0 instead of 0-200g)\n",
    "3. Unequal learning (large targets dominate gradients)\n",
    "\n",
    "**The solution: Normalize targets + use plain MSE during training**\n",
    "- All targets on same scale (mean=0, std=1)\n",
    "- Equal gradient contribution\n",
    "- Model outputs reasonable range\n",
    "- Evaluate with competition weights (where they belong!)\n",
    "\n",
    "## This Experiment\n",
    "\n",
    "We'll train **3 models** with different loss functions:\n",
    "\n",
    "### **Approach A: Normalized + Plain MSE** (RECOMMENDED)\n",
    "- Normalize targets to mean=0, std=1\n",
    "- Train with unweighted MSE\n",
    "- Denormalize predictions for evaluation\n",
    "\n",
    "### **Approach B: Plain MSE + Output Scaling**\n",
    "- No target normalization\n",
    "- Add ReLU to ensure positive outputs\n",
    "- Train with unweighted MSE\n",
    "\n",
    "### **Approach C: Competition Weighted** (CURRENT)\n",
    "- Current approach (for comparison)\n",
    "- Weighted MSE with [0.1, 0.1, 0.1, 0.2, 0.5]\n",
    "\n",
    "**Each model:**\n",
    "1. First: Test if it can overfit a single batch (sanity check)\n",
    "2. Then: Train for 10 epochs\n",
    "3. Compare: R¬≤ scores, loss curves, predictions\n",
    "\n",
    "**Expected winner: Approach A**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from PIL import Image\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "from tqdm.auto import tqdm\n",
    "import copy\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "\n",
    "print(\"‚úì Imports complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train_enriched = pd.read_csv('competition/train_enriched.csv')\n",
    "train_enriched['Sampling_Date'] = pd.to_datetime(train_enriched['Sampling_Date'])\n",
    "train_enriched['full_image_path'] = train_enriched['image_path'].apply(lambda x: f'competition/{x}')\n",
    "\n",
    "target_cols = ['Dry_Green_g', 'Dry_Dead_g', 'Dry_Clover_g', 'GDM_g', 'Dry_Total_g']\n",
    "competition_weights = torch.tensor([0.1, 0.1, 0.1, 0.2, 0.5])\n",
    "\n",
    "train_data, val_data = train_test_split(train_enriched, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Data loaded: {len(train_data)} train, {len(val_data)} val\")\n",
    "print(f\"Targets: {target_cols}\")\n",
    "print(f\"Competition weights: {competition_weights.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate target statistics for normalization\n",
    "target_means = torch.tensor([train_data[col].mean() for col in target_cols], dtype=torch.float32)\n",
    "target_stds = torch.tensor([train_data[col].std() for col in target_cols], dtype=torch.float32)\n",
    "\n",
    "print(\"\\nTarget statistics:\")\n",
    "for i, col in enumerate(target_cols):\n",
    "    print(f\"  {col:15s}: mean={target_means[i]:.2f}g, std={target_stds[i]:.2f}g\")\n",
    "\n",
    "# Save for later use\n",
    "torch.save({'means': target_means, 'stds': target_stds}, 'target_normalization.pth')\n",
    "print(\"\\n‚úì Target normalization stats saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Define 3 Dataset Classes (Normalized vs Unnormalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalizedDataset(Dataset):\n",
    "    \"\"\"Dataset with NORMALIZED targets (Approach A).\"\"\"\n",
    "    def __init__(self, dataframe, target_means, target_stds, augment=False):\n",
    "        self.df = dataframe.reset_index(drop=True)\n",
    "        self.target_means = target_means\n",
    "        self.target_stds = target_stds\n",
    "        \n",
    "        if augment:\n",
    "            self.transform = transforms.Compose([\n",
    "                transforms.Resize((224, 224)),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.RandomVerticalFlip(),\n",
    "                transforms.RandomRotation(10),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "            ])\n",
    "        else:\n",
    "            self.transform = transforms.Compose([\n",
    "                transforms.Resize((224, 224)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "            ])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img = Image.open(row['full_image_path']).convert('RGB')\n",
    "        img = self.transform(img)\n",
    "        \n",
    "        # Normalize targets\n",
    "        targets = torch.tensor(row[target_cols].values.astype('float32'), dtype=torch.float32)\n",
    "        targets_normalized = (targets - self.target_means) / self.target_stds\n",
    "        \n",
    "        return {\n",
    "            'image': img, \n",
    "            'targets': targets_normalized,  # Normalized targets\n",
    "            'targets_original': targets      # Keep original for evaluation\n",
    "        }\n",
    "\n",
    "class UnnormalizedDataset(Dataset):\n",
    "    \"\"\"Dataset with UNNORMALIZED targets (Approaches B and C).\"\"\"\n",
    "    def __init__(self, dataframe, augment=False):\n",
    "        self.df = dataframe.reset_index(drop=True)\n",
    "        \n",
    "        if augment:\n",
    "            self.transform = transforms.Compose([\n",
    "                transforms.Resize((224, 224)),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.RandomVerticalFlip(),\n",
    "                transforms.RandomRotation(10),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "            ])\n",
    "        else:\n",
    "            self.transform = transforms.Compose([\n",
    "                transforms.Resize((224, 224)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "            ])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img = Image.open(row['full_image_path']).convert('RGB')\n",
    "        img = self.transform(img)\n",
    "        targets = torch.tensor(row[target_cols].values.astype('float32'), dtype=torch.float32)\n",
    "        \n",
    "        return {'image': img, 'targets': targets}\n",
    "\n",
    "print(\"‚úì Dataset classes defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Define 3 Model Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelA_Normalized(nn.Module):\n",
    "    \"\"\"Approach A: For normalized targets (outputs can be negative).\"\"\"\n",
    "    def __init__(self, num_outputs=5):\n",
    "        super().__init__()\n",
    "        self.resnet = models.resnet18(pretrained=True)\n",
    "        num_features = self.resnet.fc.in_features\n",
    "        \n",
    "        # Simple FC head - NO activation at end (can output negative)\n",
    "        self.resnet.fc = nn.Sequential(\n",
    "            nn.Linear(num_features, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, num_outputs)  # No final activation!\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.resnet(x)\n",
    "\n",
    "class ModelB_PlainMSE(nn.Module):\n",
    "    \"\"\"Approach B: For unnormalized targets with ReLU (ensure positive).\"\"\"\n",
    "    def __init__(self, num_outputs=5):\n",
    "        super().__init__()\n",
    "        self.resnet = models.resnet18(pretrained=True)\n",
    "        num_features = self.resnet.fc.in_features\n",
    "        \n",
    "        # FC head with ReLU at end (ensure positive outputs)\n",
    "        self.resnet.fc = nn.Sequential(\n",
    "            nn.Linear(num_features, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, num_outputs),\n",
    "            nn.ReLU()  # Ensure positive (biomass can't be negative!)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.resnet(x)\n",
    "\n",
    "class ModelC_Weighted(nn.Module):\n",
    "    \"\"\"Approach C: Current approach (for comparison).\"\"\"\n",
    "    def __init__(self, num_outputs=5):\n",
    "        super().__init__()\n",
    "        self.resnet = models.resnet18(pretrained=True)\n",
    "        num_features = self.resnet.fc.in_features\n",
    "        \n",
    "        # Same as Model B\n",
    "        self.resnet.fc = nn.Sequential(\n",
    "            nn.Linear(num_features, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, num_outputs),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.resnet(x)\n",
    "\n",
    "print(\"‚úì Model architectures defined\")\n",
    "print(\"\\n  Model A: For normalized targets (no final activation)\")\n",
    "print(\"  Model B: For unnormalized targets (ReLU at end)\")\n",
    "print(\"  Model C: Same as B (difference is loss function)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Define 3 Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlainMSELoss(nn.Module):\n",
    "    \"\"\"Approach A & B: Plain unweighted MSE.\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, pred, target):\n",
    "        return F.mse_loss(pred, target)\n",
    "\n",
    "class CompetitionWeightedLoss(nn.Module):\n",
    "    \"\"\"Approach C: Competition-weighted MSE (current approach).\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.weights = competition_weights.to(device)\n",
    "    \n",
    "    def forward(self, pred, target):\n",
    "        mse = F.mse_loss(pred, target, reduction='none')\n",
    "        weighted_mse = (mse * self.weights).mean()\n",
    "        return weighted_mse\n",
    "\n",
    "print(\"‚úì Loss functions defined\")\n",
    "print(\"\\n  PlainMSELoss: Simple MSE (for A & B)\")\n",
    "print(\"  CompetitionWeightedLoss: Weighted MSE with [0.1, 0.1, 0.1, 0.2, 0.5] (for C)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Evaluation Function (Always uses Competition Weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_competition_r2(predictions, targets):\n",
    "    \"\"\"\n",
    "    Calculate competition R¬≤ score (weighted).\n",
    "    \n",
    "    Args:\n",
    "        predictions: numpy array (N, 5)\n",
    "        targets: numpy array (N, 5)\n",
    "    \n",
    "    Returns:\n",
    "        competition_r2: float (weighted R¬≤)\n",
    "        per_target_r2: list of floats (R¬≤ per target)\n",
    "    \"\"\"\n",
    "    per_target_r2 = []\n",
    "    competition_r2 = 0\n",
    "    \n",
    "    weights = competition_weights.numpy()\n",
    "    \n",
    "    for i in range(5):\n",
    "        r2 = r2_score(targets[:, i], predictions[:, i])\n",
    "        per_target_r2.append(r2)\n",
    "        competition_r2 += weights[i] * r2\n",
    "    \n",
    "    return competition_r2, per_target_r2\n",
    "\n",
    "print(\"‚úì Evaluation function defined\")\n",
    "print(\"  Always evaluates with competition weights [0.1, 0.1, 0.1, 0.2, 0.5]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Test 1: Can Each Model Overfit a Single Batch?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_overfitting(model, dataset, loss_fn, model_name, is_normalized=False, num_steps=100):\n",
    "    \"\"\"\n",
    "    Test if model can overfit a single batch.\n",
    "    \n",
    "    Args:\n",
    "        is_normalized: If True, denormalize predictions for R¬≤ calculation\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"OVERFITTING TEST: {model_name}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Get one batch\n",
    "    loader = DataLoader(dataset, batch_size=16, shuffle=False)\n",
    "    batch = next(iter(loader))\n",
    "    images = batch['image'].to(device)\n",
    "    targets = batch['targets'].to(device)\n",
    "    \n",
    "    if is_normalized:\n",
    "        targets_original = batch['targets_original'].to(device)\n",
    "    else:\n",
    "        targets_original = targets\n",
    "    \n",
    "    # Train for num_steps\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)  # Higher LR for overfitting\n",
    "    \n",
    "    losses = []\n",
    "    r2_scores = []\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "        # Forward\n",
    "        pred = model(images)\n",
    "        loss = loss_fn(pred, targets)\n",
    "        \n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # Gradient clipping\n",
    "        optimizer.step()\n",
    "        \n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        # Calculate R¬≤ every 20 steps\n",
    "        if step % 20 == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                pred_eval = model(images)\n",
    "                \n",
    "                # Denormalize if needed\n",
    "                if is_normalized:\n",
    "                    pred_denorm = pred_eval * target_stds.to(device) + target_means.to(device)\n",
    "                else:\n",
    "                    pred_denorm = pred_eval\n",
    "                \n",
    "                comp_r2, _ = calculate_competition_r2(\n",
    "                    pred_denorm.cpu().numpy(), \n",
    "                    targets_original.cpu().numpy()\n",
    "                )\n",
    "                r2_scores.append(comp_r2)\n",
    "                print(f\"Step {step:3d}: Loss = {loss.item():.4f}, R¬≤ = {comp_r2:+.4f}\")\n",
    "            model.train()\n",
    "    \n",
    "    # Final evaluation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        pred_final = model(images)\n",
    "        \n",
    "        if is_normalized:\n",
    "            pred_final_denorm = pred_final * target_stds.to(device) + target_means.to(device)\n",
    "        else:\n",
    "            pred_final_denorm = pred_final\n",
    "        \n",
    "        final_r2, per_target_r2 = calculate_competition_r2(\n",
    "            pred_final_denorm.cpu().numpy(),\n",
    "            targets_original.cpu().numpy()\n",
    "        )\n",
    "    \n",
    "    print(f\"\\nFinal Results:\")\n",
    "    print(f\"  Competition R¬≤: {final_r2:+.4f}\")\n",
    "    print(f\"\\n  Per-target R¬≤:\")\n",
    "    for i, col in enumerate(target_cols):\n",
    "        print(f\"    {col:15s}: {per_target_r2[i]:+.4f}\")\n",
    "    \n",
    "    # Interpretation\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    if final_r2 > 0.9:\n",
    "        print(\"‚úÖ SUCCESS: Model CAN overfit a single batch!\")\n",
    "        print(\"   Architecture is working. Ready for full training.\")\n",
    "        success = True\n",
    "    elif final_r2 > 0.5:\n",
    "        print(\"‚ö†Ô∏è  PARTIAL: Model learning but slowly.\")\n",
    "        print(\"   May work with more epochs or tuning.\")\n",
    "        success = True\n",
    "    elif final_r2 > 0.0:\n",
    "        print(\"‚ö†Ô∏è  WEAK: Model barely learning.\")\n",
    "        print(\"   Will likely struggle in full training.\")\n",
    "        success = False\n",
    "    else:\n",
    "        print(\"‚ùå FAILURE: Model CANNOT learn even a single batch.\")\n",
    "        print(\"   Architecture or loss function is broken.\")\n",
    "        success = False\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    return success, final_r2, losses, r2_scores\n",
    "\n",
    "print(\"‚úì Overfitting test function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "train_dataset_normalized = NormalizedDataset(train_data, target_means, target_stds, augment=False)\n",
    "train_dataset_unnormalized = UnnormalizedDataset(train_data, augment=False)\n",
    "\n",
    "# Test Approach A: Normalized + Plain MSE\n",
    "model_a = ModelA_Normalized()\n",
    "loss_a = PlainMSELoss()\n",
    "success_a, r2_a, losses_a, r2_hist_a = test_overfitting(\n",
    "    model_a, train_dataset_normalized, loss_a, \n",
    "    \"Approach A: Normalized + Plain MSE\",\n",
    "    is_normalized=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Approach B: Plain MSE + Output Scaling\n",
    "model_b = ModelB_PlainMSE()\n",
    "loss_b = PlainMSELoss()\n",
    "success_b, r2_b, losses_b, r2_hist_b = test_overfitting(\n",
    "    model_b, train_dataset_unnormalized, loss_b,\n",
    "    \"Approach B: Plain MSE + Output Scaling (ReLU)\",\n",
    "    is_normalized=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Approach C: Competition Weighted\n",
    "model_c = ModelC_Weighted()\n",
    "loss_c = CompetitionWeightedLoss()\n",
    "success_c, r2_c, losses_c, r2_hist_c = test_overfitting(\n",
    "    model_c, train_dataset_unnormalized, loss_c,\n",
    "    \"Approach C: Competition Weighted MSE (Current)\",\n",
    "    is_normalized=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare overfitting results\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"OVERFITTING TEST SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "results_df = pd.DataFrame({\n",
    "    'Approach': [\n",
    "        'A: Normalized + Plain MSE',\n",
    "        'B: Plain MSE + ReLU',\n",
    "        'C: Competition Weighted'\n",
    "    ],\n",
    "    'Final R¬≤ (100 steps)': [r2_a, r2_b, r2_c],\n",
    "    'Can Overfit?': [\n",
    "        '‚úÖ Yes' if success_a else '‚ùå No',\n",
    "        '‚úÖ Yes' if success_b else '‚ùå No',\n",
    "        '‚úÖ Yes' if success_c else '‚ùå No'\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"\\n\" + results_df.to_string(index=False))\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "# Determine if we should proceed with full training\n",
    "if not (success_a or success_b or success_c):\n",
    "    print(\"\\n‚ö†Ô∏è  WARNING: None of the approaches can overfit a single batch!\")\n",
    "    print(\"   This suggests a fundamental problem.\")\n",
    "    print(\"   Recommend investigating further before full training.\")\n",
    "elif success_a:\n",
    "    print(\"\\n‚úÖ Approach A can overfit! This is the most promising approach.\")\n",
    "    print(\"   Proceeding with full training...\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ At least one approach can overfit.\")\n",
    "    print(\"   Proceeding with full training for successful approach(es)...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Full Training: 10 Epochs Each\n",
    "\n",
    "Only train approaches that passed the overfitting test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, loss_fn, num_epochs=10, \n",
    "                model_name='Model', is_normalized=False):\n",
    "    \"\"\"\n",
    "    Train model for specified epochs.\n",
    "    \n",
    "    Args:\n",
    "        is_normalized: If True, denormalize predictions for R¬≤ evaluation\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"TRAINING: {model_name}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)\n",
    "    \n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'val_loss': [],\n",
    "        'val_r2': [],\n",
    "        'epoch': []\n",
    "    }\n",
    "    \n",
    "    best_r2 = -float('inf')\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            images = batch['image'].to(device)\n",
    "            targets = batch['targets'].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item() * images.size(0)\n",
    "        \n",
    "        train_loss /= len(train_loader.dataset)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        all_preds = []\n",
    "        all_targets = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                images = batch['image'].to(device)\n",
    "                targets = batch['targets'].to(device)\n",
    "                \n",
    "                if is_normalized:\n",
    "                    targets_original = batch['targets_original'].to(device)\n",
    "                else:\n",
    "                    targets_original = targets\n",
    "                \n",
    "                outputs = model(images)\n",
    "                loss = loss_fn(outputs, targets)\n",
    "                val_loss += loss.item() * images.size(0)\n",
    "                \n",
    "                # Denormalize for R¬≤ calculation\n",
    "                if is_normalized:\n",
    "                    outputs_denorm = outputs * target_stds.to(device) + target_means.to(device)\n",
    "                else:\n",
    "                    outputs_denorm = outputs\n",
    "                \n",
    "                all_preds.append(outputs_denorm.cpu().numpy())\n",
    "                all_targets.append(targets_original.cpu().numpy())\n",
    "        \n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        \n",
    "        # Calculate R¬≤\n",
    "        all_preds = np.vstack(all_preds)\n",
    "        all_targets = np.vstack(all_targets)\n",
    "        val_r2, _ = calculate_competition_r2(all_preds, all_targets)\n",
    "        \n",
    "        # Update scheduler\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # Store history\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_r2'].append(val_r2)\n",
    "        history['epoch'].append(epoch + 1)\n",
    "        \n",
    "        # Print progress\n",
    "        print(f\"Epoch {epoch+1:2d}/{num_epochs}: Train Loss={train_loss:.4f}, Val Loss={val_loss:.4f}, Val R¬≤={val_r2:+.4f}\")\n",
    "        \n",
    "        # Save best\n",
    "        if val_r2 > best_r2:\n",
    "            best_r2 = val_r2\n",
    "            torch.save(model.state_dict(), f'{model_name.replace(\" \", \"_\")}_best.pth')\n",
    "            print(f\"  üíæ New best R¬≤ = {best_r2:+.4f}\")\n",
    "    \n",
    "    print(f\"\\n‚úì Training complete! Best R¬≤ = {best_r2:+.4f}\\n\")\n",
    "    return history, best_r2\n",
    "\n",
    "print(\"‚úì Training function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloaders\n",
    "batch_size = 16\n",
    "\n",
    "# For Approach A (normalized)\n",
    "train_dataset_norm = NormalizedDataset(train_data, target_means, target_stds, augment=True)\n",
    "val_dataset_norm = NormalizedDataset(val_data, target_means, target_stds, augment=False)\n",
    "train_loader_norm = DataLoader(train_dataset_norm, batch_size=batch_size, shuffle=True)\n",
    "val_loader_norm = DataLoader(val_dataset_norm, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# For Approaches B & C (unnormalized)\n",
    "train_dataset_unnorm = UnnormalizedDataset(train_data, augment=True)\n",
    "val_dataset_unnorm = UnnormalizedDataset(val_data, augment=False)\n",
    "train_loader_unnorm = DataLoader(train_dataset_unnorm, batch_size=batch_size, shuffle=True)\n",
    "val_loader_unnorm = DataLoader(val_dataset_unnorm, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(\"‚úì Dataloaders created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Approach A (if passed overfitting test)\n",
    "if success_a:\n",
    "    model_a_full = ModelA_Normalized()\n",
    "    history_a, best_r2_a = train_model(\n",
    "        model_a_full, train_loader_norm, val_loader_norm, loss_a,\n",
    "        num_epochs=10, model_name='Approach_A', is_normalized=True\n",
    "    )\n",
    "else:\n",
    "    print(\"‚è≠Ô∏è  Skipping Approach A (failed overfitting test)\")\n",
    "    history_a, best_r2_a = None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Approach B (if passed overfitting test)\n",
    "if success_b:\n",
    "    model_b_full = ModelB_PlainMSE()\n",
    "    history_b, best_r2_b = train_model(\n",
    "        model_b_full, train_loader_unnorm, val_loader_unnorm, loss_b,\n",
    "        num_epochs=10, model_name='Approach_B', is_normalized=False\n",
    "    )\n",
    "else:\n",
    "    print(\"‚è≠Ô∏è  Skipping Approach B (failed overfitting test)\")\n",
    "    history_b, best_r2_b = None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Approach C (if passed overfitting test)\n",
    "if success_c:\n",
    "    model_c_full = ModelC_Weighted()\n",
    "    history_c, best_r2_c = train_model(\n",
    "        model_c_full, train_loader_unnorm, val_loader_unnorm, loss_c,\n",
    "        num_epochs=10, model_name='Approach_C', is_normalized=False\n",
    "    )\n",
    "else:\n",
    "    print(\"‚è≠Ô∏è  Skipping Approach C (failed overfitting test)\")\n",
    "    history_c, best_r2_c = None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Comparison & Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final comparison\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Approach': [\n",
    "        'A: Normalized + Plain MSE',\n",
    "        'B: Plain MSE + ReLU',\n",
    "        'C: Competition Weighted',\n",
    "        '---',\n",
    "        'Linear Regression (baseline)',\n",
    "        'Previous CNN (40 epochs)'\n",
    "    ],\n",
    "    'Competition R¬≤': [\n",
    "        f\"{best_r2_a:+.4f}\" if best_r2_a is not None else 'N/A',\n",
    "        f\"{best_r2_b:+.4f}\" if best_r2_b is not None else 'N/A',\n",
    "        f\"{best_r2_c:+.4f}\" if best_r2_c is not None else 'N/A',\n",
    "        '---',\n",
    "        '+0.2048',\n",
    "        '-1.2527'\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"\\n\" + comparison_df.to_string(index=False))\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss curves\n",
    "ax = axes[0]\n",
    "if history_a:\n",
    "    ax.plot(history_a['epoch'], history_a['val_loss'], 'o-', label='A: Normalized + Plain MSE', linewidth=2)\n",
    "if history_b:\n",
    "    ax.plot(history_b['epoch'], history_b['val_loss'], 's-', label='B: Plain MSE + ReLU', linewidth=2)\n",
    "if history_c:\n",
    "    ax.plot(history_c['epoch'], history_c['val_loss'], '^-', label='C: Competition Weighted', linewidth=2)\n",
    "ax.set_xlabel('Epoch', fontsize=12)\n",
    "ax.set_ylabel('Validation Loss', fontsize=12)\n",
    "ax.set_title('Loss Curves', fontsize=14, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "# R¬≤ curves\n",
    "ax = axes[1]\n",
    "if history_a:\n",
    "    ax.plot(history_a['epoch'], history_a['val_r2'], 'o-', label='A: Normalized + Plain MSE', linewidth=2)\n",
    "if history_b:\n",
    "    ax.plot(history_b['epoch'], history_b['val_r2'], 's-', label='B: Plain MSE + ReLU', linewidth=2)\n",
    "if history_c:\n",
    "    ax.plot(history_c['epoch'], history_c['val_r2'], '^-', label='C: Competition Weighted', linewidth=2)\n",
    "ax.axhline(y=0.0, color='gray', linestyle='--', linewidth=2, label='Baseline (mean)')\n",
    "ax.axhline(y=0.2048, color='orange', linestyle='--', linewidth=2, label='Linear regression')\n",
    "ax.set_xlabel('Epoch', fontsize=12)\n",
    "ax.set_ylabel('Competition R¬≤', fontsize=12)\n",
    "ax.set_title('R¬≤ Progress', fontsize=14, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('loss_function_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Comparison plot saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Conclusion & Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CONCLUSION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Determine winner\n",
    "valid_approaches = []\n",
    "if best_r2_a is not None:\n",
    "    valid_approaches.append(('A', best_r2_a))\n",
    "if best_r2_b is not None:\n",
    "    valid_approaches.append(('B', best_r2_b))\n",
    "if best_r2_c is not None:\n",
    "    valid_approaches.append(('C', best_r2_c))\n",
    "\n",
    "if valid_approaches:\n",
    "    winner, winner_score = max(valid_approaches, key=lambda x: x[1])\n",
    "    \n",
    "    print(f\"\\nüèÜ WINNER: Approach {winner}\")\n",
    "    print(f\"   Competition R¬≤ = {winner_score:+.4f}\")\n",
    "    \n",
    "    if winner_score > 0.2:\n",
    "        print(\"\\n‚úÖ SUCCESS! CNN beats linear regression!\")\n",
    "        print(\"\\n   Key insights:\")\n",
    "        if winner == 'A':\n",
    "            print(\"   ‚Ä¢ Target normalization is CRITICAL for this task\")\n",
    "            print(\"   ‚Ä¢ Plain MSE works better than weighted MSE for training\")\n",
    "            print(\"   ‚Ä¢ Competition weights belong in EVALUATION, not TRAINING\")\n",
    "        elif winner == 'B':\n",
    "            print(\"   ‚Ä¢ Output scaling (ReLU) helps with unnormalized targets\")\n",
    "            print(\"   ‚Ä¢ Plain MSE works better than weighted MSE\")\n",
    "        else:\n",
    "            print(\"   ‚Ä¢ Weighted MSE can work (surprising!)\")\n",
    "        \n",
    "        print(\"\\n   Next steps:\")\n",
    "        print(\"   1. Scale up to 20-30 epochs with early stopping\")\n",
    "        print(\"   2. Try slightly larger model (ResNet34)\")\n",
    "        print(\"   3. Fine-tune learning rate and weight decay\")\n",
    "        print(\"   4. Generate test predictions and submit!\")\n",
    "        \n",
    "    elif winner_score > 0.0:\n",
    "        print(\"\\n‚ö†Ô∏è  PARTIAL SUCCESS: Positive R¬≤ but below linear regression\")\n",
    "        print(\"\\n   Recommendations:\")\n",
    "        print(\"   ‚Ä¢ Train for more epochs (20-30)\")\n",
    "        print(\"   ‚Ä¢ Try custom normalization (not ImageNet)\")\n",
    "        print(\"   ‚Ä¢ Experiment with learning rate\")\n",
    "        print(\"   ‚Ä¢ Consider ensemble with linear model\")\n",
    "        \n",
    "    else:\n",
    "        print(\"\\n‚ùå FAILURE: All approaches still have negative R¬≤\")\n",
    "        print(\"\\n   Further investigation needed:\")\n",
    "        print(\"   ‚Ä¢ Check ImageNet normalization (use custom?)\")\n",
    "        print(\"   ‚Ä¢ Try training from scratch (no pretrained weights)\")\n",
    "        print(\"   ‚Ä¢ Investigate data quality issues\")\n",
    "        print(\"   ‚Ä¢ Consider simpler task (predict only Dry_Total_g)\")\n",
    "else:\n",
    "    print(\"\\n‚ùå All approaches failed overfitting test\")\n",
    "    print(\"   Need to fix fundamental architecture issues first\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úì Experiment complete!\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
