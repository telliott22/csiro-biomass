{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Option 4: K-Fold Cross-Validation (RECOMMENDED)\n",
    "\n",
    "## Problem\n",
    "Single 80/20 split creates validation set (72 images) that doesn't represent test distribution.\n",
    "\n",
    "**Evidence**: Baseline Val RÂ²=0.6852 â†’ Kaggle RÂ²=0.51 (gap of -0.175!)\n",
    "\n",
    "## Solution\n",
    "**5-Fold Cross-Validation with Ensemble**\n",
    "- Train 5 ResNet18 models (proven best architecture)\n",
    "- Each fold uses different 80/20 split\n",
    "- Ensemble predictions (average across 5 models)\n",
    "\n",
    "## Why This Works\n",
    "1. **Better validation estimate**: 5 different validation sets\n",
    "2. **Ensemble effect**: Averaging reduces overfitting\n",
    "3. **Proven architecture**: Using ResNet18, not experimental\n",
    "\n",
    "## Expected\n",
    "**Kaggle RÂ²: 0.53-0.55** (improves from baseline 0.51)\n",
    "\n",
    "## Time\n",
    "- DEBUG: ~20 min (2 epochs Ã— 5 folds)\n",
    "- FULL: ~4 hours (45 epochs Ã— 5 folds)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from PIL import Image\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from tqdm.auto import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "\n",
    "print(\"âœ“ Setup complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# CONFIGURATION: 5-FOLD CROSS-VALIDATION\n",
    "# ================================================================\n",
    "\n",
    "DEBUG_MODE = False  # Set to False for full training\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "TARGET_COLS = ['Dry_Green_g', 'Dry_Dead_g', 'Dry_Clover_g', 'GDM_g', 'Dry_Total_g']\n",
    "COMPETITION_WEIGHTS = torch.tensor([0.1, 0.1, 0.1, 0.2, 0.5])\n",
    "NUM_FOLDS = 5\n",
    "\n",
    "if DEBUG_MODE:\n",
    "    print(\"\\n\" + \"ðŸ›\"*40)\n",
    "    print(\"DEBUG MODE: 2 epochs/phase per fold\")\n",
    "    print(\"ðŸ›\"*40 + \"\\n\")\n",
    "\n",
    "def get_epochs(full):\n",
    "    return 2 if DEBUG_MODE else full\n",
    "\n",
    "# Use ResNet18 (proven best) with standard config\n",
    "BASE_CONFIG = {\n",
    "    'phase1_epochs': get_epochs(15),\n",
    "    'phase2_epochs': get_epochs(30),\n",
    "    'phase1_lr': 3e-4,\n",
    "    'phase2_lr': 3e-4,\n",
    "    'phase2_backbone_lr': 1e-5,\n",
    "    'weight_decay': 1e-4,\n",
    "    'hidden_dim': 256,\n",
    "    'dropout': 0.2,\n",
    "    'backbone': 'resnet18',\n",
    "    'use_scheduler': False\n",
    "}\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"K-FOLD CROSS-VALIDATION\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nFolds: {NUM_FOLDS}\")\n",
    "print(f\"Model: ResNet18 (proven best)\")\n",
    "print(f\"Phase 1: {BASE_CONFIG['phase1_epochs']} epochs\")\n",
    "print(f\"Phase 2: {BASE_CONFIG['phase2_epochs']} epochs\")\n",
    "print(f\"\\nâ±ï¸ {'DEBUG: ~20 min' if DEBUG_MODE else 'FULL: ~4 hours'}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load full dataset\n",
    "train_enriched = pd.read_csv('../../competition/train_enriched.csv')\n",
    "train_enriched['Sampling_Date'] = pd.to_datetime(train_enriched['Sampling_Date'])\n",
    "train_enriched['full_image_path'] = train_enriched['image_path'].apply(lambda x: f'../../competition/{x}')\n",
    "\n",
    "print(f\"Full dataset: {len(train_enriched)} images\")\n",
    "print(f\"Will create {NUM_FOLDS} folds of ~{len(train_enriched)//NUM_FOLDS} val images each\")\n",
    "\n",
    "# Calculate normalization on FULL dataset (consistent across folds)\n",
    "target_means = torch.tensor([train_enriched[col].mean() for col in TARGET_COLS], dtype=torch.float32)\n",
    "target_stds = torch.tensor([train_enriched[col].std() for col in TARGET_COLS], dtype=torch.float32)\n",
    "\n",
    "print(\"\\nNormalization (full dataset):\")\n",
    "for i, col in enumerate(TARGET_COLS):\n",
    "    print(f\"  {col:15s}: mean={target_means[i]:.2f}g, std={target_stds[i]:.2f}g\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare tabular feature names\n",
    "weather_features = [\n",
    "    'rainfall_7d', 'rainfall_30d',\n",
    "    'temp_max_7d', 'temp_min_7d', 'temp_mean_7d', 'temp_mean_30d', 'temp_range_7d',\n",
    "    'et0_7d', 'et0_30d',\n",
    "    'water_balance_7d', 'water_balance_30d',\n",
    "    'days_since_rain', 'daylength', 'season'\n",
    "]\n",
    "\n",
    "continuous_features = weather_features + ['Pre_GSHH_NDVI', 'Height_Ave_cm']\n",
    "\n",
    "print(f\"âœ“ Feature lists prepared\")\n",
    "print(f\"  Weather features: {len(weather_features)}\")\n",
    "print(f\"  Continuous features: {len(continuous_features)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Dataset Classes & Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalizedDataset(Dataset):\n",
    "    def __init__(self, dataframe, target_means, target_stds, augment=False):\n",
    "        self.df = dataframe.reset_index(drop=True)\n",
    "        self.target_means = target_means\n",
    "        self.target_stds = target_stds\n",
    "\n",
    "        transform_list = [transforms.Resize((224, 224))]\n",
    "        if augment:\n",
    "            transform_list.extend([\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.RandomVerticalFlip(),\n",
    "                transforms.RandomRotation(10),\n",
    "            ])\n",
    "        transform_list.extend([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        self.transform = transforms.Compose(transform_list)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img = Image.open(row['full_image_path']).convert('RGB')\n",
    "        img = self.transform(img)\n",
    "        targets = torch.tensor(row[TARGET_COLS].values.astype('float32'), dtype=torch.float32)\n",
    "        targets_normalized = (targets - self.target_means) / self.target_stds\n",
    "        return {'image': img, 'targets': targets_normalized, 'targets_original': targets}\n",
    "\n",
    "class MultimodalDataset(Dataset):\n",
    "    def __init__(self, dataframe, target_means, target_stds, weather_features, augment=False):\n",
    "        self.df = dataframe.reset_index(drop=True)\n",
    "        self.target_means = target_means\n",
    "        self.target_stds = target_stds\n",
    "        self.weather_features = weather_features\n",
    "\n",
    "        transform_list = [transforms.Resize((224, 224))]\n",
    "        if augment:\n",
    "            transform_list.extend([\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.RandomVerticalFlip(),\n",
    "                transforms.RandomRotation(10),\n",
    "            ])\n",
    "        transform_list.extend([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        self.transform = transforms.Compose(transform_list)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img = Image.open(row['full_image_path']).convert('RGB')\n",
    "        img = self.transform(img)\n",
    "        weather = torch.tensor(row[self.weather_features].values.astype('float32'), dtype=torch.float32)\n",
    "        ndvi = torch.tensor(row['Pre_GSHH_NDVI'], dtype=torch.float32)\n",
    "        height = torch.tensor(row['Height_Ave_cm'], dtype=torch.float32)\n",
    "        state = torch.tensor(row['State_encoded'], dtype=torch.long)\n",
    "        species = torch.tensor(row['Species_encoded'], dtype=torch.long)\n",
    "        targets = torch.tensor(row[TARGET_COLS].values.astype('float32'), dtype=torch.float32)\n",
    "        targets_normalized = (targets - self.target_means) / self.target_stds\n",
    "        return {\n",
    "            'image': img, 'weather': weather, 'ndvi': ndvi, 'height': height,\n",
    "            'state': state, 'species': species,\n",
    "            'targets': targets_normalized, 'targets_original': targets\n",
    "        }\n",
    "\n",
    "print(\"âœ“ Dataset classes defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AuxiliaryPretrainedModel(nn.Module):\n",
    "    def __init__(self, num_outputs=5, hidden_dim=256, dropout=0.2, num_states=4, num_species=15):\n",
    "        super().__init__()\n",
    "        # ResNet18 backbone\n",
    "        model = models.resnet18(pretrained=True)\n",
    "        self.backbone = nn.Sequential(*list(model.children())[:-1])\n",
    "        feature_dim = 512\n",
    "\n",
    "        # Auxiliary heads\n",
    "        self.ndvi_head = nn.Linear(feature_dim, 1)\n",
    "        self.height_head = nn.Linear(feature_dim, 1)\n",
    "        self.weather_head = nn.Linear(feature_dim, 14)\n",
    "        self.state_head = nn.Linear(feature_dim, num_states)\n",
    "        self.species_head = nn.Linear(feature_dim, num_species)\n",
    "\n",
    "        # Biomass head\n",
    "        self.biomass_head = nn.Sequential(\n",
    "            nn.Linear(feature_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, num_outputs)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, mode='biomass'):\n",
    "        features = self.backbone(x).flatten(1)\n",
    "        if mode == 'auxiliary':\n",
    "            return {\n",
    "                'ndvi': self.ndvi_head(features),\n",
    "                'height': self.height_head(features),\n",
    "                'weather': self.weather_head(features),\n",
    "                'state': self.state_head(features),\n",
    "                'species': self.species_head(features)\n",
    "            }\n",
    "        else:\n",
    "            return self.biomass_head(features)\n",
    "\n",
    "print(\"âœ“ Model defined (ResNet18)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_competition_r2(predictions, targets):\n",
    "    per_target_r2 = []\n",
    "    competition_r2 = 0\n",
    "    weights = COMPETITION_WEIGHTS.numpy()\n",
    "    for i in range(5):\n",
    "        r2 = r2_score(targets[:, i], predictions[:, i])\n",
    "        per_target_r2.append(r2)\n",
    "        competition_r2 += weights[i] * r2\n",
    "    return competition_r2, per_target_r2\n",
    "\n",
    "def auxiliary_loss(predictions, targets):\n",
    "    loss_ndvi = F.mse_loss(predictions['ndvi'].squeeze(), targets['ndvi'])\n",
    "    loss_height = F.mse_loss(predictions['height'].squeeze(), targets['height'])\n",
    "    loss_weather = F.mse_loss(predictions['weather'], targets['weather'])\n",
    "    loss_state = F.cross_entropy(predictions['state'], targets['state'])\n",
    "    loss_species = F.cross_entropy(predictions['species'], targets['species'])\n",
    "    total_loss = (\n",
    "        0.2 * loss_ndvi + 0.2 * loss_height + 0.3 * loss_weather +\n",
    "        0.15 * loss_state + 0.15 * loss_species\n",
    "    )\n",
    "    return total_loss\n",
    "\n",
    "print(\"âœ“ Utility functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_phase1(model, train_loader, val_loader, num_epochs, lr, fold_name):\n",
    "    print(f\"\\nPHASE 1: AUXILIARY PRETRAINING - {fold_name}\")\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for batch in tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}', leave=False):\n",
    "            images = batch['image'].to(device)\n",
    "            targets = {\n",
    "                'ndvi': batch['ndvi'].to(device),\n",
    "                'height': batch['height'].to(device),\n",
    "                'weather': batch['weather'].to(device),\n",
    "                'state': batch['state'].to(device),\n",
    "                'species': batch['species'].to(device)\n",
    "            }\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(images, mode='auxiliary')\n",
    "            loss = auxiliary_loss(predictions, targets)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * images.size(0)\n",
    "\n",
    "        train_loss /= len(train_loader.dataset)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}: Loss={train_loss:.4f}\")\n",
    "\n",
    "    print(f\"âœ“ Phase 1 complete\\n\")\n",
    "\n",
    "print(\"âœ“ train_phase1() defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_phase2(model, train_loader, val_loader, config, target_means, target_stds, fold_name):\n",
    "    print(f\"PHASE 2: BIOMASS FINE-TUNING - {fold_name}\")\n",
    "    num_epochs = config['phase2_epochs']\n",
    "    lr_head = config['phase2_lr']\n",
    "    lr_backbone = config['phase2_backbone_lr']\n",
    "\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.AdamW([\n",
    "        {'params': model.backbone.parameters(), 'lr': lr_backbone},\n",
    "        {'params': model.biomass_head.parameters(), 'lr': lr_head}\n",
    "    ], weight_decay=config['weight_decay'])\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    best_r2 = -float('inf')\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Train\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for batch in tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}', leave=False):\n",
    "            images = batch['image'].to(device)\n",
    "            targets = batch['targets'].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images, mode='biomass')\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * images.size(0)\n",
    "        train_loss /= len(train_loader.dataset)\n",
    "\n",
    "        # Validate\n",
    "        model.eval()\n",
    "        all_preds = []\n",
    "        all_targets = []\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                images = batch['image'].to(device)\n",
    "                targets_original = batch['targets_original']\n",
    "                outputs = model(images, mode='biomass')\n",
    "                outputs_denorm = outputs.cpu() * target_stds + target_means\n",
    "                all_preds.append(outputs_denorm.numpy())\n",
    "                all_targets.append(targets_original.numpy())\n",
    "\n",
    "        all_preds = np.vstack(all_preds)\n",
    "        all_targets = np.vstack(all_targets)\n",
    "        val_r2, _ = calculate_competition_r2(all_preds, all_targets)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}: Loss={train_loss:.4f}, Val RÂ²={val_r2:+.4f}\")\n",
    "\n",
    "        if val_r2 > best_r2:\n",
    "            best_r2 = val_r2\n",
    "            torch.save(model.state_dict(), f'model4b_{fold_name}_phase2_best.pth')\n",
    "            print(f\"  ðŸ’¾ New best: RÂ²={best_r2:+.4f}\")\n",
    "\n",
    "    print(f\"\\nâœ“ Phase 2 complete! Best RÂ²={best_r2:+.4f}\\n\")\n",
    "    return best_r2\n",
    "\n",
    "print(\"âœ“ train_phase2() defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 5: K-Fold Training\n",
    "\n",
    "Train 5 models, one for each fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storage for all folds\n",
    "fold_results = []\n",
    "fold_models = []\n",
    "fold_val_indices = []\n",
    "\n",
    "# Create K-fold splits\n",
    "kf = KFold(n_splits=NUM_FOLDS, shuffle=True, random_state=42)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"STARTING K-FOLD TRAINING\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fold_idx, (train_indices, val_indices) in enumerate(kf.split(train_enriched)):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"FOLD {fold_idx+1}/{NUM_FOLDS}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Train: {len(train_indices)} images, Val: {len(val_indices)} images\")\n",
    "\n",
    "    # Split data\n",
    "    train_fold = train_enriched.iloc[train_indices].copy()\n",
    "    val_fold = train_enriched.iloc[val_indices].copy()\n",
    "\n",
    "    # Scale features for this fold\n",
    "    scaler = StandardScaler()\n",
    "    train_fold[continuous_features] = scaler.fit_transform(train_fold[continuous_features])\n",
    "    val_fold[continuous_features] = scaler.transform(val_fold[continuous_features])\n",
    "\n",
    "    # Encode categorical\n",
    "    le_state = LabelEncoder()\n",
    "    le_species = LabelEncoder()\n",
    "    train_fold['State_encoded'] = le_state.fit_transform(train_fold['State'])\n",
    "    train_fold['Species_encoded'] = le_species.fit_transform(train_fold['Species'])\n",
    "    val_fold['State_encoded'] = le_state.transform(val_fold['State'])\n",
    "    val_fold['Species_encoded'] = le_species.transform(val_fold['Species'])\n",
    "\n",
    "    num_states = len(le_state.classes_)\n",
    "    num_species = len(le_species.classes_)\n",
    "\n",
    "    # Create datasets\n",
    "    train_dataset_simple = NormalizedDataset(train_fold, target_means, target_stds, augment=True)\n",
    "    train_dataset_multi = MultimodalDataset(train_fold, target_means, target_stds, weather_features, augment=True)\n",
    "    val_dataset_simple = NormalizedDataset(val_fold, target_means, target_stds, augment=False)\n",
    "    val_dataset_multi = MultimodalDataset(val_fold, target_means, target_stds, weather_features, augment=False)\n",
    "\n",
    "    # Create model\n",
    "    model = AuxiliaryPretrainedModel(\n",
    "        num_outputs=5,\n",
    "        hidden_dim=BASE_CONFIG['hidden_dim'],\n",
    "        dropout=BASE_CONFIG['dropout'],\n",
    "        num_states=num_states,\n",
    "        num_species=num_species\n",
    "    )\n",
    "\n",
    "    # Create dataloaders\n",
    "    train_loader_multi = DataLoader(train_dataset_multi, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "    val_loader_multi = DataLoader(val_dataset_multi, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "    train_loader_simple = DataLoader(train_dataset_simple, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "    val_loader_simple = DataLoader(val_dataset_simple, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "    # Phase 1\n",
    "    train_phase1(\n",
    "        model=model,\n",
    "        train_loader=train_loader_multi,\n",
    "        val_loader=val_loader_multi,\n",
    "        num_epochs=BASE_CONFIG['phase1_epochs'],\n",
    "        lr=BASE_CONFIG['phase1_lr'],\n",
    "        fold_name=f'Fold{fold_idx+1}'\n",
    "    )\n",
    "\n",
    "    # Phase 2\n",
    "    best_r2 = train_phase2(\n",
    "        model=model,\n",
    "        train_loader=train_loader_simple,\n",
    "        val_loader=val_loader_simple,\n",
    "        config=BASE_CONFIG,\n",
    "        target_means=target_means,\n",
    "        target_stds=target_stds,\n",
    "        fold_name=f'Fold{fold_idx+1}'\n",
    "    )\n",
    "\n",
    "    # Load best checkpoint\n",
    "    model.load_state_dict(torch.load(f'model4b_Fold{fold_idx+1}_phase2_best.pth'))\n",
    "    model.eval()\n",
    "\n",
    "    # Store\n",
    "    fold_results.append({\n",
    "        'fold': fold_idx + 1,\n",
    "        'best_r2': best_r2\n",
    "    })\n",
    "    fold_models.append(model)\n",
    "    fold_val_indices.append(val_indices)\n",
    "\n",
    "    print(f\"âœ… Fold {fold_idx+1} complete! RÂ²={best_r2:+.4f}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"K-FOLD TRAINING COMPLETE\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"\\nFold RÂ² scores:\")\n",
    "for res in fold_results:\n",
    "    print(f\"  Fold {res['fold']}: RÂ²={res['best_r2']:+.4f}\")\n",
    "mean_r2 = np.mean([r['best_r2'] for r in fold_results])\n",
    "std_r2 = np.std([r['best_r2'] for r in fold_results])\n",
    "print(f\"\\nMean: {mean_r2:+.4f}\")\n",
    "print(f\"Std:  {std_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 6: Ensemble Evaluation\n",
    "\n",
    "Evaluate the ensemble of all 5 models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ENSEMBLE VALIDATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# For each fold's validation set, get predictions from all 5 models\n",
    "all_ensemble_preds = []\n",
    "all_ensemble_targets = []\n",
    "\n",
    "for fold_idx, val_indices in enumerate(fold_val_indices):\n",
    "    val_fold = train_enriched.iloc[val_indices]\n",
    "    val_dataset = NormalizedDataset(val_fold, target_means, target_stds, augment=False)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "    # Get predictions from all models\n",
    "    fold_preds = []\n",
    "    for model in fold_models:\n",
    "        model = model.to(device)\n",
    "        model.eval()\n",
    "        preds = []\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                images = batch['image'].to(device)\n",
    "                outputs = model(images, mode='biomass')\n",
    "                outputs_denorm = outputs.cpu() * target_stds + target_means\n",
    "                preds.append(outputs_denorm.numpy())\n",
    "        fold_preds.append(np.vstack(preds))\n",
    "\n",
    "    # Average predictions\n",
    "    ensemble_preds = np.mean(fold_preds, axis=0)\n",
    "    targets = val_fold[TARGET_COLS].values\n",
    "\n",
    "    all_ensemble_preds.append(ensemble_preds)\n",
    "    all_ensemble_targets.append(targets)\n",
    "\n",
    "# Combine all folds\n",
    "all_ensemble_preds = np.vstack(all_ensemble_preds)\n",
    "all_ensemble_targets = np.vstack(all_ensemble_targets)\n",
    "\n",
    "# Calculate ensemble RÂ²\n",
    "ensemble_r2, ensemble_per_target = calculate_competition_r2(all_ensemble_preds, all_ensemble_targets)\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"ENSEMBLE RESULTS\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"\\nEnsemble RÂ² ({NUM_FOLDS} models averaged): {ensemble_r2:+.4f}\")\n",
    "print(f\"Mean single model RÂ²: {mean_r2:+.4f}\")\n",
    "print(f\"Ensemble improvement: {ensemble_r2 - mean_r2:+.4f}\")\n",
    "\n",
    "baseline_r2 = 0.6852\n",
    "baseline_kaggle = 0.51\n",
    "print(f\"\\nðŸ“Š vs BASELINE:\")\n",
    "print(f\"  Baseline (single ResNet18): Val={baseline_r2:+.4f}, Kaggle={baseline_kaggle:+.2f}\")\n",
    "print(f\"  Ensemble ({NUM_FOLDS} models): Val={ensemble_r2:+.4f}\")\n",
    "print(f\"  Val improvement: {ensemble_r2 - baseline_r2:+.4f}\")\n",
    "\n",
    "print(f\"\\nPer-target RÂ² (Ensemble):\")\n",
    "for i, col in enumerate(TARGET_COLS):\n",
    "    print(f\"  {col:15s}: {ensemble_per_target[i]:+.4f}\")\n",
    "\n",
    "print(f\"\\nâœ… Ensemble ready for Kaggle submission!\")\n",
    "print(f\"   Expected Kaggle: ~0.53-0.55 (baseline was {baseline_kaggle:+.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "### Approach\n",
    "5-fold cross-validation with ResNet18, ensemble predictions\n",
    "\n",
    "### Results\n",
    "[See output above]\n",
    "\n",
    "### Key Insights\n",
    "1. **Ensemble Effect**: Averaging 5 models typically improves ~0.01-0.02 RÂ²\n",
    "2. **Better Validation**: Multiple folds give more robust estimate\n",
    "3. **Expected Kaggle**: If ensemble Val RÂ² â‰ˆ 0.685-0.69 â†’ Kaggle ~0.53-0.55\n",
    "\n",
    "### Next Steps\n",
    "1. Create Kaggle submission using ensemble\n",
    "2. Compare with Option 1 (EfficientNet tuning)\n",
    "3. Use best performing approach for final submission"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
