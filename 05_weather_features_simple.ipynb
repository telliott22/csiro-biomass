{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weather Feature Engineering (Simplified)\n",
    "\n",
    "Adding weather features without climatology (to avoid rate limits)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from weather_api import WeatherAPIClient, calculate_daylength, get_southern_hemisphere_season\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('competition/train.csv')\n",
    "train_wide = train_df.pivot_table(\n",
    "    index=['image_path', 'Sampling_Date', 'State', 'Species', 'Pre_GSHH_NDVI', 'Height_Ave_cm'],\n",
    "    columns='target_name',\n",
    "    values='target'\n",
    ").reset_index()\n",
    "\n",
    "train_wide['Sampling_Date'] = pd.to_datetime(train_wide['Sampling_Date'])\n",
    "\n",
    "print(f\"Training samples: {len(train_wide)}\")\n",
    "print(f\"States: {train_wide['State'].value_counts().to_dict()}\")\n",
    "print(f\"Date range: {train_wide['Sampling_Date'].min()} to {train_wide['Sampling_Date'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Fetch Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_client = WeatherAPIClient(cache_dir='weather_cache')\n",
    "\n",
    "min_date = train_wide['Sampling_Date'].min()\n",
    "max_date = train_wide['Sampling_Date'].max()\n",
    "\n",
    "print(\"Fetching weather data...\\n\")\n",
    "weather_data = {}\n",
    "for state in train_wide['State'].unique():\n",
    "    print(f\"{state}:\")\n",
    "    df = weather_client.fetch_weather_data(\n",
    "        state=state,\n",
    "        start_date=min_date.strftime('%Y-%m-%d'),\n",
    "        end_date=max_date.strftime('%Y-%m-%d'),\n",
    "        days_before=30\n",
    "    )\n",
    "    weather_data[state] = df\n",
    "\n",
    "print(\"\\n✓ Weather data loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Calculate All Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enrich_weather_data(df):\n",
    "    \"\"\"Add all weather features to a state's weather dataframe.\"\"\"\n",
    "    df = df.copy().sort_values('date')\n",
    "    \n",
    "    # Rolling features\n",
    "    df['rainfall_7d'] = df['precipitation'].rolling(7, min_periods=1).sum()\n",
    "    df['rainfall_30d'] = df['precipitation'].rolling(30, min_periods=1).sum()\n",
    "    df['temp_max_7d'] = df['temp_max'].rolling(7, min_periods=1).mean()\n",
    "    df['temp_min_7d'] = df['temp_min'].rolling(7, min_periods=1).mean()\n",
    "    df['temp_mean_7d'] = df['temp_mean'].rolling(7, min_periods=1).mean()\n",
    "    df['temp_mean_30d'] = df['temp_mean'].rolling(30, min_periods=1).mean()\n",
    "    df['temp_range_7d'] = (df['temp_max'] - df['temp_min']).rolling(7, min_periods=1).mean()\n",
    "    df['et0_7d'] = df['et0'].rolling(7, min_periods=1).sum()\n",
    "    df['et0_30d'] = df['et0'].rolling(30, min_periods=1).sum()\n",
    "    \n",
    "    # Water balance\n",
    "    df['water_balance_7d'] = df['rainfall_7d'] - df['et0_7d']\n",
    "    df['water_balance_30d'] = df['rainfall_30d'] - df['et0_30d']\n",
    "    \n",
    "    # Days since rain\n",
    "    days_counter = 0\n",
    "    days_list = []\n",
    "    for precip in df['precipitation']:\n",
    "        if precip > 5:\n",
    "            days_counter = 0\n",
    "        else:\n",
    "            days_counter += 1\n",
    "        days_list.append(days_counter)\n",
    "    df['days_since_rain'] = days_list\n",
    "    \n",
    "    # Daylength and season\n",
    "    df['daylength'] = df.apply(lambda row: calculate_daylength(row['lat'], row['date']), axis=1)\n",
    "    df['season'] = df['date'].apply(get_southern_hemisphere_season)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply to all states\n",
    "print(\"Calculating features...\\n\")\n",
    "for state in weather_data.keys():\n",
    "    weather_data[state] = enrich_weather_data(weather_data[state])\n",
    "    print(f\"✓ {state}\")\n",
    "\n",
    "print(\"\\n✓ All features calculated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Merge with Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine weather data\n",
    "all_weather = pd.concat(weather_data.values(), ignore_index=True)\n",
    "\n",
    "# Select features\n",
    "weather_features = [\n",
    "    'rainfall_7d', 'rainfall_30d',\n",
    "    'temp_max_7d', 'temp_min_7d', 'temp_mean_7d', 'temp_mean_30d', 'temp_range_7d',\n",
    "    'et0_7d', 'et0_30d',\n",
    "    'water_balance_7d', 'water_balance_30d',\n",
    "    'days_since_rain',\n",
    "    'daylength', 'season'\n",
    "]\n",
    "\n",
    "weather_for_merge = all_weather[['date', 'state'] + weather_features].copy()\n",
    "weather_for_merge.columns = ['Sampling_Date', 'State'] + weather_features\n",
    "\n",
    "# Merge\n",
    "train_enriched = train_wide.merge(weather_for_merge, on=['Sampling_Date', 'State'], how='left')\n",
    "\n",
    "print(f\"Original columns: {train_wide.shape[1]}\")\n",
    "print(f\"Enriched columns: {train_enriched.shape[1]}\")\n",
    "print(f\"New features: {len(weather_features)}\")\n",
    "print(f\"\\nFeatures added: {weather_features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Add NDVI Anomaly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate NDVI stats by state\n",
    "ndvi_stats = train_enriched.groupby('State')['Pre_GSHH_NDVI'].agg(['mean', 'std']).reset_index()\n",
    "ndvi_stats.columns = ['State', 'ndvi_mean', 'ndvi_std']\n",
    "\n",
    "train_enriched = train_enriched.merge(ndvi_stats, on='State')\n",
    "train_enriched['ndvi_anomaly'] = (\n",
    "    (train_enriched['Pre_GSHH_NDVI'] - train_enriched['ndvi_mean']) / train_enriched['ndvi_std']\n",
    ")\n",
    "\n",
    "all_features = weather_features + ['ndvi_anomaly']\n",
    "print(f\"Total new features: {len(all_features)}\")\n",
    "print(f\"\\nAll features: {all_features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Weather feature summary:\\n\")\n",
    "print(train_enriched[all_features].describe().T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Correlation with Targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_cols = ['Dry_Green_g', 'Dry_Dead_g', 'Dry_Clover_g', 'GDM_g', 'Dry_Total_g']\n",
    "\n",
    "# Key features for correlation\n",
    "key_features = [\n",
    "    'rainfall_7d', 'rainfall_30d', 'temp_mean_7d', 'et0_7d',\n",
    "    'water_balance_7d', 'days_since_rain', 'daylength', 'ndvi_anomaly'\n",
    "]\n",
    "\n",
    "corr = train_enriched[key_features + target_cols].corr()\n",
    "target_corr = corr.loc[key_features, target_cols]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(target_corr, annot=True, fmt='.2f', cmap='coolwarm', center=0)\n",
    "plt.title('Weather Feature Correlations with Biomass Targets')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTop correlations with Dry_Total_g:\")\n",
    "print(target_corr['Dry_Total_g'].abs().sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Enriched Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "output_file = 'competition/train_enriched.csv'\n",
    "train_enriched.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"✓ Saved to {output_file}\")\n",
    "print(f\"Shape: {train_enriched.shape}\")\n",
    "print(f\"New features: {len(all_features)}\")\n",
    "print(f\"\\nMissing values: {train_enriched[all_features].isnull().sum().sum()}\")\n",
    "\n",
    "# Preview\n",
    "print(\"\\nSample data:\")\n",
    "train_enriched[['image_path', 'State', 'Sampling_Date'] + all_features[:5]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**15 Weather Features Added:**\n",
    "- Rainfall: 7d, 30d totals\n",
    "- Temperature: 7d max/min/mean, 30d mean, 7d range\n",
    "- Evapotranspiration: 7d, 30d totals\n",
    "- Water balance: 7d, 30d (rain - ET)\n",
    "- Days since last significant rain\n",
    "- Daylength, Season\n",
    "- NDVI anomaly\n",
    "\n",
    "**Ready for modeling!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
