{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Exploration: Understanding Biomass Prediction\n",
    "\n",
    "**Goal**: Before training more neural networks, let's understand:\n",
    "1. What visual features predict biomass?\n",
    "2. Are there correlations between color and biomass values?\n",
    "3. Can simple models (linear regression on color) predict biomass?\n",
    "4. Are there data quality issues?\n",
    "\n",
    "**Why this matters**: If simple color features don't correlate with biomass, then:\n",
    "- ColorJitter is definitely hurting training (scrambling important signal)\n",
    "- We need to understand what features the CNN should learn\n",
    "- Might have data quality issues to fix first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1: Setup & Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy import stats\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"‚úì Imports complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load enriched training data\n",
    "train_enriched = pd.read_csv('competition/train_enriched.csv')\n",
    "train_enriched['Sampling_Date'] = pd.to_datetime(train_enriched['Sampling_Date'])\n",
    "train_enriched['full_image_path'] = train_enriched['image_path'].apply(lambda x: f'competition/{x}')\n",
    "\n",
    "# Define target columns\n",
    "target_cols = ['Dry_Green_g', 'Dry_Dead_g', 'Dry_Clover_g', 'GDM_g', 'Dry_Total_g']\n",
    "competition_weights = [0.1, 0.1, 0.1, 0.2, 0.5]\n",
    "\n",
    "print(f\"Total samples: {len(train_enriched)}\")\n",
    "print(f\"Shape: {train_enriched.shape}\")\n",
    "print(f\"\\nTarget columns: {target_cols}\")\n",
    "print(f\"Competition weights: {competition_weights}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: Target Variable Distributions\n",
    "\n",
    "Let's understand the distribution of biomass values we're trying to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics for all targets\n",
    "print(\"=\"*80)\n",
    "print(\"TARGET VARIABLE STATISTICS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for col in target_cols:\n",
    "    print(f\"\\n{col}:\")\n",
    "    print(f\"  Mean: {train_enriched[col].mean():.2f}g\")\n",
    "    print(f\"  Std:  {train_enriched[col].std():.2f}g\")\n",
    "    print(f\"  Min:  {train_enriched[col].min():.2f}g\")\n",
    "    print(f\"  Max:  {train_enriched[col].max():.2f}g\")\n",
    "    print(f\"  Median: {train_enriched[col].median():.2f}g\")\n",
    "    \n",
    "    # Check for zeros (common in Dry_Clover_g)\n",
    "    n_zeros = (train_enriched[col] == 0).sum()\n",
    "    pct_zeros = 100 * n_zeros / len(train_enriched)\n",
    "    print(f\"  Zeros: {n_zeros} ({pct_zeros:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution plots\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, col in enumerate(target_cols):\n",
    "    ax = axes[i]\n",
    "    \n",
    "    # Histogram\n",
    "    ax.hist(train_enriched[col], bins=30, alpha=0.7, color='steelblue', edgecolor='black')\n",
    "    ax.axvline(train_enriched[col].mean(), color='red', linestyle='--', linewidth=2, label='Mean')\n",
    "    ax.axvline(train_enriched[col].median(), color='orange', linestyle='--', linewidth=2, label='Median')\n",
    "    \n",
    "    ax.set_xlabel(f'{col} (grams)', fontsize=11)\n",
    "    ax.set_ylabel('Count', fontsize=11)\n",
    "    ax.set_title(f'Distribution of {col}', fontsize=12, fontweight='bold')\n",
    "    ax.legend()\n",
    "    ax.grid(alpha=0.3)\n",
    "\n",
    "# Remove extra subplot\n",
    "fig.delaxes(axes[5])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('target_distributions.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Target distributions plotted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3: Visual Inspection - High vs Low Biomass\n",
    "\n",
    "**Key Question**: Can we visually see the difference between high and low biomass images?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image_grid(df, title, n_images=8, figsize=(16, 4)):\n",
    "    \"\"\"Display a grid of images with their biomass values.\"\"\"\n",
    "    fig, axes = plt.subplots(1, n_images, figsize=figsize)\n",
    "    \n",
    "    for i, (idx, row) in enumerate(df.iterrows()):\n",
    "        if i >= n_images:\n",
    "            break\n",
    "            \n",
    "        # Load and display image\n",
    "        img = Image.open(row['full_image_path'])\n",
    "        axes[i].imshow(img)\n",
    "        axes[i].axis('off')\n",
    "        \n",
    "        # Add biomass info as title\n",
    "        axes[i].set_title(\n",
    "            f\"Total: {row['Dry_Total_g']:.0f}g\\n\"\n",
    "            f\"Green: {row['Dry_Green_g']:.0f}g\\n\"\n",
    "            f\"Dead: {row['Dry_Dead_g']:.0f}g\",\n",
    "            fontsize=9\n",
    "        )\n",
    "    \n",
    "    plt.suptitle(title, fontsize=14, fontweight='bold', y=1.02)\n",
    "    plt.tight_layout()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# High Dry_Total_g images (should be dense/green)\n",
    "high_biomass = train_enriched.nlargest(8, 'Dry_Total_g')\n",
    "fig = show_image_grid(high_biomass, 'HIGHEST Dry_Total_g Images (Top 8)')\n",
    "plt.savefig('high_biomass_images.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"High biomass stats:\")\n",
    "print(f\"  Mean Dry_Total_g: {high_biomass['Dry_Total_g'].mean():.0f}g\")\n",
    "print(f\"  Mean NDVI: {high_biomass['Pre_GSHH_NDVI'].mean():.3f}\")\n",
    "print(f\"  Mean Height: {high_biomass['Height_Ave_cm'].mean():.1f}cm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Low Dry_Total_g images (should be sparse/brown)\n",
    "low_biomass = train_enriched.nsmallest(8, 'Dry_Total_g')\n",
    "fig = show_image_grid(low_biomass, 'LOWEST Dry_Total_g Images (Bottom 8)')\n",
    "plt.savefig('low_biomass_images.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Low biomass stats:\")\n",
    "print(f\"  Mean Dry_Total_g: {low_biomass['Dry_Total_g'].mean():.0f}g\")\n",
    "print(f\"  Mean NDVI: {low_biomass['Pre_GSHH_NDVI'].mean():.3f}\")\n",
    "print(f\"  Mean Height: {low_biomass['Height_Ave_cm'].mean():.1f}cm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# High Dry_Green_g images (should be very green)\n",
    "high_green = train_enriched.nlargest(8, 'Dry_Green_g')\n",
    "fig = show_image_grid(high_green, 'HIGHEST Dry_Green_g Images (Green Vegetation)')\n",
    "plt.savefig('high_green_images.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# High Dry_Dead_g images (should be brown/dead)\n",
    "high_dead = train_enriched.nlargest(8, 'Dry_Dead_g')\n",
    "fig = show_image_grid(high_dead, 'HIGHEST Dry_Dead_g Images (Dead Vegetation)')\n",
    "plt.savefig('high_dead_images.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 4: Color Feature Extraction\n",
    "\n",
    "Extract RGB and HSV statistics from all images to see if they correlate with biomass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_color_features(image_path, resize=(224, 224)):\n",
    "    \"\"\"Extract RGB and HSV color statistics from an image.\"\"\"\n",
    "    try:\n",
    "        # Load and resize image\n",
    "        img = Image.open(image_path).convert('RGB')\n",
    "        img = img.resize(resize)\n",
    "        \n",
    "        # Convert to numpy array\n",
    "        img_array = np.array(img) / 255.0  # Normalize to [0, 1]\n",
    "        \n",
    "        # RGB features\n",
    "        r_mean = img_array[:, :, 0].mean()\n",
    "        g_mean = img_array[:, :, 1].mean()\n",
    "        b_mean = img_array[:, :, 2].mean()\n",
    "        \n",
    "        r_std = img_array[:, :, 0].std()\n",
    "        g_std = img_array[:, :, 1].std()\n",
    "        b_std = img_array[:, :, 2].std()\n",
    "        \n",
    "        # Color ratios (useful for vegetation)\n",
    "        green_red_ratio = g_mean / (r_mean + 1e-6)\n",
    "        green_blue_ratio = g_mean / (b_mean + 1e-6)\n",
    "        \n",
    "        # Convert to HSV\n",
    "        img_hsv = img.convert('HSV')\n",
    "        hsv_array = np.array(img_hsv) / 255.0\n",
    "        \n",
    "        h_mean = hsv_array[:, :, 0].mean()\n",
    "        s_mean = hsv_array[:, :, 1].mean()\n",
    "        v_mean = hsv_array[:, :, 2].mean()\n",
    "        \n",
    "        # Overall brightness and variance\n",
    "        brightness = img_array.mean()\n",
    "        variance = img_array.std()\n",
    "        \n",
    "        return {\n",
    "            'r_mean': r_mean,\n",
    "            'g_mean': g_mean,\n",
    "            'b_mean': b_mean,\n",
    "            'r_std': r_std,\n",
    "            'g_std': g_std,\n",
    "            'b_std': b_std,\n",
    "            'green_red_ratio': green_red_ratio,\n",
    "            'green_blue_ratio': green_blue_ratio,\n",
    "            'h_mean': h_mean,\n",
    "            's_mean': s_mean,\n",
    "            'v_mean': v_mean,\n",
    "            'brightness': brightness,\n",
    "            'variance': variance\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {image_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "print(\"‚úì Color feature extraction function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract color features from all images\n",
    "print(\"Extracting color features from all images...\")\n",
    "print(\"This may take 2-3 minutes for 357 images...\\n\")\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "color_features_list = []\n",
    "for idx, row in tqdm(train_enriched.iterrows(), total=len(train_enriched)):\n",
    "    features = extract_color_features(row['full_image_path'])\n",
    "    if features:\n",
    "        features['index'] = idx\n",
    "        color_features_list.append(features)\n",
    "\n",
    "# Create dataframe\n",
    "color_features_df = pd.DataFrame(color_features_list)\n",
    "color_features_df.set_index('index', inplace=True)\n",
    "\n",
    "# Merge with original data\n",
    "data_with_features = train_enriched.join(color_features_df)\n",
    "\n",
    "print(f\"\\n‚úì Extracted {len(color_features_df)} color feature sets\")\n",
    "print(f\"\\nColor features: {list(color_features_df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick look at color features\n",
    "print(\"Color Feature Statistics:\")\n",
    "print(\"=\"*60)\n",
    "print(color_features_df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 5: Correlation Analysis\n",
    "\n",
    "**Critical Question**: Do color features correlate with biomass targets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlations between color features and biomass targets\n",
    "color_feature_cols = list(color_features_df.columns)\n",
    "tabular_feature_cols = ['Pre_GSHH_NDVI', 'Height_Ave_cm', 'temp_mean_7d', 'rainfall_7d']\n",
    "all_feature_cols = color_feature_cols + tabular_feature_cols\n",
    "\n",
    "# Create correlation matrix\n",
    "correlation_data = data_with_features[all_feature_cols + target_cols]\n",
    "correlation_matrix = correlation_data.corr()\n",
    "\n",
    "# Extract correlations with targets only\n",
    "target_correlations = correlation_matrix[target_cols].loc[all_feature_cols]\n",
    "\n",
    "print(\"Top 10 Correlations with Each Target:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for target in target_cols:\n",
    "    print(f\"\\n{target}:\")\n",
    "    top_corr = target_correlations[target].abs().sort_values(ascending=False).head(10)\n",
    "    for feature, corr_val in top_corr.items():\n",
    "        actual_corr = target_correlations.loc[feature, target]\n",
    "        print(f\"  {feature:20s}: {actual_corr:+.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap of correlations\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 8))\n",
    "\n",
    "# Full correlation heatmap (features vs targets)\n",
    "ax = axes[0]\n",
    "sns.heatmap(target_correlations, annot=False, cmap='coolwarm', center=0, \n",
    "            vmin=-1, vmax=1, ax=ax, cbar_kws={'label': 'Correlation'})\n",
    "ax.set_title('Correlations: Features vs Biomass Targets', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Biomass Targets', fontsize=12)\n",
    "ax.set_ylabel('Features', fontsize=12)\n",
    "\n",
    "# Target intercorrelations\n",
    "ax = axes[1]\n",
    "target_intercorr = correlation_matrix.loc[target_cols, target_cols]\n",
    "sns.heatmap(target_intercorr, annot=True, fmt='.2f', cmap='coolwarm', center=0,\n",
    "            vmin=-1, vmax=1, ax=ax, square=True, cbar_kws={'label': 'Correlation'})\n",
    "ax.set_title('Biomass Target Intercorrelations', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('', fontsize=12)\n",
    "ax.set_ylabel('', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('correlation_heatmaps.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Correlation heatmaps plotted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 6: Scatter Plots - Visual Correlations\n",
    "\n",
    "Visualize the strongest correlations to understand relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find strongest correlations for each target\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, target in enumerate(target_cols):\n",
    "    ax = axes[i]\n",
    "    \n",
    "    # Find feature with strongest correlation\n",
    "    abs_corr = target_correlations[target].abs()\n",
    "    strongest_feature = abs_corr.idxmax()\n",
    "    corr_value = target_correlations.loc[strongest_feature, target]\n",
    "    \n",
    "    # Scatter plot\n",
    "    ax.scatter(data_with_features[strongest_feature], \n",
    "              data_with_features[target],\n",
    "              alpha=0.5, s=30, color='steelblue')\n",
    "    \n",
    "    # Add trend line\n",
    "    z = np.polyfit(data_with_features[strongest_feature], data_with_features[target], 1)\n",
    "    p = np.poly1d(z)\n",
    "    x_line = np.linspace(data_with_features[strongest_feature].min(), \n",
    "                         data_with_features[strongest_feature].max(), 100)\n",
    "    ax.plot(x_line, p(x_line), \"r--\", linewidth=2, label=f'Trend (r={corr_value:.3f})')\n",
    "    \n",
    "    ax.set_xlabel(strongest_feature, fontsize=11)\n",
    "    ax.set_ylabel(target, fontsize=11)\n",
    "    ax.set_title(f'{target} vs {strongest_feature}', fontsize=12, fontweight='bold')\n",
    "    ax.legend()\n",
    "    ax.grid(alpha=0.3)\n",
    "\n",
    "# Remove extra subplot\n",
    "fig.delaxes(axes[5])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('strongest_correlations.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Strongest correlation scatter plots created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specific scatter plots: Color features vs biomass\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "# Green channel vs Dry_Green_g\n",
    "ax = axes[0, 0]\n",
    "ax.scatter(data_with_features['g_mean'], data_with_features['Dry_Green_g'], \n",
    "          alpha=0.5, s=30, color='green')\n",
    "corr = data_with_features[['g_mean', 'Dry_Green_g']].corr().iloc[0, 1]\n",
    "ax.set_title(f'Green Channel vs Dry_Green_g (r={corr:.3f})', fontweight='bold')\n",
    "ax.set_xlabel('Mean Green Channel')\n",
    "ax.set_ylabel('Dry_Green_g')\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "# Red channel vs Dry_Dead_g\n",
    "ax = axes[0, 1]\n",
    "ax.scatter(data_with_features['r_mean'], data_with_features['Dry_Dead_g'], \n",
    "          alpha=0.5, s=30, color='brown')\n",
    "corr = data_with_features[['r_mean', 'Dry_Dead_g']].corr().iloc[0, 1]\n",
    "ax.set_title(f'Red Channel vs Dry_Dead_g (r={corr:.3f})', fontweight='bold')\n",
    "ax.set_xlabel('Mean Red Channel')\n",
    "ax.set_ylabel('Dry_Dead_g')\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "# Green/Red ratio vs Dry_Total_g\n",
    "ax = axes[0, 2]\n",
    "ax.scatter(data_with_features['green_red_ratio'], data_with_features['Dry_Total_g'], \n",
    "          alpha=0.5, s=30, color='olive')\n",
    "corr = data_with_features[['green_red_ratio', 'Dry_Total_g']].corr().iloc[0, 1]\n",
    "ax.set_title(f'Green/Red Ratio vs Dry_Total_g (r={corr:.3f})', fontweight='bold')\n",
    "ax.set_xlabel('Green/Red Ratio')\n",
    "ax.set_ylabel('Dry_Total_g')\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "# NDVI vs Dry_Green_g\n",
    "ax = axes[1, 0]\n",
    "ax.scatter(data_with_features['Pre_GSHH_NDVI'], data_with_features['Dry_Green_g'], \n",
    "          alpha=0.5, s=30, color='darkgreen')\n",
    "corr = data_with_features[['Pre_GSHH_NDVI', 'Dry_Green_g']].corr().iloc[0, 1]\n",
    "ax.set_title(f'NDVI vs Dry_Green_g (r={corr:.3f})', fontweight='bold')\n",
    "ax.set_xlabel('Pre_GSHH_NDVI')\n",
    "ax.set_ylabel('Dry_Green_g')\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "# NDVI vs Dry_Total_g\n",
    "ax = axes[1, 1]\n",
    "ax.scatter(data_with_features['Pre_GSHH_NDVI'], data_with_features['Dry_Total_g'], \n",
    "          alpha=0.5, s=30, color='navy')\n",
    "corr = data_with_features[['Pre_GSHH_NDVI', 'Dry_Total_g']].corr().iloc[0, 1]\n",
    "ax.set_title(f'NDVI vs Dry_Total_g (r={corr:.3f})', fontweight='bold')\n",
    "ax.set_xlabel('Pre_GSHH_NDVI')\n",
    "ax.set_ylabel('Dry_Total_g')\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "# Height vs Dry_Total_g\n",
    "ax = axes[1, 2]\n",
    "ax.scatter(data_with_features['Height_Ave_cm'], data_with_features['Dry_Total_g'], \n",
    "          alpha=0.5, s=30, color='purple')\n",
    "corr = data_with_features[['Height_Ave_cm', 'Dry_Total_g']].corr().iloc[0, 1]\n",
    "ax.set_title(f'Height vs Dry_Total_g (r={corr:.3f})', fontweight='bold')\n",
    "ax.set_xlabel('Height_Ave_cm')\n",
    "ax.set_ylabel('Dry_Total_g')\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('color_vs_biomass_scatter.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Color vs biomass scatter plots created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 7: Simple Baseline Models\n",
    "\n",
    "**Key Question**: What R¬≤ can we achieve with simple linear regression?\n",
    "\n",
    "This gives us a baseline to beat with neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Train/val split (same as in main notebook)\n",
    "train_data, val_data = train_test_split(data_with_features, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training samples: {len(train_data)}\")\n",
    "print(f\"Validation samples: {len(val_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_linear_model(X_train, y_train, X_val, y_val, feature_names, model_name):\n",
    "    \"\"\"Train and evaluate a linear regression model.\"\"\"\n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "    \n",
    "    # Train model\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Predict\n",
    "    y_pred_train = model.predict(X_train_scaled)\n",
    "    y_pred_val = model.predict(X_val_scaled)\n",
    "    \n",
    "    # Calculate R¬≤ for each target\n",
    "    results = {}\n",
    "    competition_score = 0\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"{model_name}\")\n",
    "    print(f\"Features: {', '.join(feature_names)}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    for i, target in enumerate(target_cols):\n",
    "        r2_train = r2_score(y_train[:, i], y_pred_train[:, i])\n",
    "        r2_val = r2_score(y_val[:, i], y_pred_val[:, i])\n",
    "        mae_val = mean_absolute_error(y_val[:, i], y_pred_val[:, i])\n",
    "        \n",
    "        results[target] = {\n",
    "            'r2_train': r2_train,\n",
    "            'r2_val': r2_val,\n",
    "            'mae_val': mae_val\n",
    "        }\n",
    "        \n",
    "        competition_score += competition_weights[i] * r2_val\n",
    "        \n",
    "        print(f\"\\n{target}:\")\n",
    "        print(f\"  Train R¬≤: {r2_train:+.4f}\")\n",
    "        print(f\"  Val R¬≤:   {r2_val:+.4f}\")\n",
    "        print(f\"  Val MAE:  {mae_val:.2f}g\")\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Competition Score: {competition_score:.4f}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    return results, competition_score\n",
    "\n",
    "print(\"‚úì Evaluation function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 1: NDVI only (Upper bound for image-only models)\n",
    "X_train_ndvi = train_data[['Pre_GSHH_NDVI']].values\n",
    "X_val_ndvi = val_data[['Pre_GSHH_NDVI']].values\n",
    "y_train = train_data[target_cols].values\n",
    "y_val = val_data[target_cols].values\n",
    "\n",
    "ndvi_results, ndvi_score = evaluate_linear_model(\n",
    "    X_train_ndvi, y_train, X_val_ndvi, y_val,\n",
    "    ['Pre_GSHH_NDVI'],\n",
    "    \"MODEL 1: NDVI Only\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 2: Color features only\n",
    "X_train_color = train_data[color_feature_cols].values\n",
    "X_val_color = val_data[color_feature_cols].values\n",
    "\n",
    "color_results, color_score = evaluate_linear_model(\n",
    "    X_train_color, y_train, X_val_color, y_val,\n",
    "    color_feature_cols[:3],  # Show first 3 for brevity\n",
    "    \"MODEL 2: Color Features Only (RGB, HSV, ratios)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 3: Simple RGB only (most basic)\n",
    "simple_rgb_cols = ['r_mean', 'g_mean', 'b_mean']\n",
    "X_train_rgb = train_data[simple_rgb_cols].values\n",
    "X_val_rgb = val_data[simple_rgb_cols].values\n",
    "\n",
    "rgb_results, rgb_score = evaluate_linear_model(\n",
    "    X_train_rgb, y_train, X_val_rgb, y_val,\n",
    "    simple_rgb_cols,\n",
    "    \"MODEL 3: Simple RGB Only (r_mean, g_mean, b_mean)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 4: Color + NDVI + Height (What a CNN should learn)\n",
    "X_train_combined = train_data[color_feature_cols + ['Pre_GSHH_NDVI', 'Height_Ave_cm']].values\n",
    "X_val_combined = val_data[color_feature_cols + ['Pre_GSHH_NDVI', 'Height_Ave_cm']].values\n",
    "\n",
    "combined_results, combined_score = evaluate_linear_model(\n",
    "    X_train_combined, y_train, X_val_combined, y_val,\n",
    "    ['Color features', 'NDVI', 'Height'],\n",
    "    \"MODEL 4: Combined (Color + NDVI + Height)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary comparison\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SIMPLE MODEL COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "summary_df = pd.DataFrame({\n",
    "    'Model': [\n",
    "        'NDVI Only',\n",
    "        'Color Features Only', \n",
    "        'Simple RGB Only',\n",
    "        'Color + NDVI + Height',\n",
    "        '---',\n",
    "        'CNN Baseline (actual)',\n",
    "    ],\n",
    "    'Competition Score': [\n",
    "        ndvi_score,\n",
    "        color_score,\n",
    "        rgb_score,\n",
    "        combined_score,\n",
    "        np.nan,\n",
    "        -1.2527  # From your actual results\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(summary_df.to_string(index=False))\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nüìä Key Insights:\")\n",
    "print(f\"  ‚Ä¢ NDVI alone achieves: {ndvi_score:.4f}\")\n",
    "print(f\"  ‚Ä¢ Color features achieve: {color_score:.4f}\")\n",
    "print(f\"  ‚Ä¢ Simple RGB achieves: {rgb_score:.4f}\")\n",
    "print(f\"  ‚Ä¢ Combined achieves: {combined_score:.4f}\")\n",
    "print(f\"  ‚Ä¢ CNN Baseline achieved: -1.2527 (WORSE than simple linear models!)\")\n",
    "\n",
    "if color_score > -1.2527:\n",
    "    print(\"\\n‚ö†Ô∏è  WARNING: Simple color features beat the CNN!\")\n",
    "    print(\"    This suggests the CNN is NOT learning properly.\")\n",
    "    print(\"    Likely causes:\")\n",
    "    print(\"      1. ColorJitter destroying color information\")\n",
    "    print(\"      2. CNN architecture too complex/overparameterized\")\n",
    "    print(\"      3. Training setup issues (learning rate, loss function)\")\n",
    "else:\n",
    "    print(\"\\n‚úì CNN baseline is better than simple linear models (as expected)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 8: Red Flags Analysis\n",
    "\n",
    "Check for data quality issues that could explain model failures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"RED FLAGS ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "red_flags = []\n",
    "\n",
    "# 1. Check if green images have low Dry_Green_g (label mismatch)\n",
    "high_green_channel = data_with_features.nlargest(20, 'g_mean')\n",
    "avg_dry_green = high_green_channel['Dry_Green_g'].mean()\n",
    "overall_avg = data_with_features['Dry_Green_g'].mean()\n",
    "\n",
    "print(f\"\\n1. Label Consistency Check:\")\n",
    "print(f\"   Images with highest green channel:\")\n",
    "print(f\"     Avg Dry_Green_g: {avg_dry_green:.2f}g\")\n",
    "print(f\"   Overall dataset:\")\n",
    "print(f\"     Avg Dry_Green_g: {overall_avg:.2f}g\")\n",
    "\n",
    "if avg_dry_green < overall_avg:\n",
    "    red_flags.append(\"‚ö†Ô∏è  Green images have LOWER Dry_Green_g than average (label mismatch?)\")\n",
    "else:\n",
    "    print(\"   ‚úì Green images have higher Dry_Green_g (labels consistent)\")\n",
    "\n",
    "# 2. Check for insufficient variance in images\n",
    "print(f\"\\n2. Image Variance Check:\")\n",
    "print(f\"   RGB variance range: {data_with_features['variance'].min():.3f} - {data_with_features['variance'].max():.3f}\")\n",
    "print(f\"   RGB variance mean: {data_with_features['variance'].mean():.3f}\")\n",
    "\n",
    "if data_with_features['variance'].std() < 0.02:\n",
    "    red_flags.append(\"‚ö†Ô∏è  Very low variance in images (all look similar)\")\n",
    "else:\n",
    "    print(\"   ‚úì Sufficient variance in images\")\n",
    "\n",
    "# 3. Check for correlation with State/Species (location dependency)\n",
    "print(f\"\\n3. Location Dependency Check:\")\n",
    "state_biomass = data_with_features.groupby('State')['Dry_Total_g'].mean()\n",
    "print(f\"   Dry_Total_g by State:\")\n",
    "for state, biomass in state_biomass.items():\n",
    "    print(f\"     {state}: {biomass:.2f}g\")\n",
    "\n",
    "if state_biomass.std() / state_biomass.mean() > 0.5:\n",
    "    red_flags.append(\"‚ö†Ô∏è  Large biomass variation by State (location-dependent, hard for CNN)\")\n",
    "else:\n",
    "    print(\"   ‚úì Biomass relatively consistent across states\")\n",
    "\n",
    "# 4. Check for outliers\n",
    "print(f\"\\n4. Outlier Check:\")\n",
    "for target in target_cols:\n",
    "    Q1 = data_with_features[target].quantile(0.25)\n",
    "    Q3 = data_with_features[target].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    outliers = ((data_with_features[target] < Q1 - 1.5*IQR) | \n",
    "                (data_with_features[target] > Q3 + 1.5*IQR)).sum()\n",
    "    pct_outliers = 100 * outliers / len(data_with_features)\n",
    "    print(f\"   {target}: {outliers} outliers ({pct_outliers:.1f}%)\")\n",
    "\n",
    "# 5. Check weak correlations\n",
    "print(f\"\\n5. Feature Correlation Strength:\")\n",
    "max_color_corr = target_correlations.loc[color_feature_cols].abs().max().max()\n",
    "ndvi_corr = target_correlations.loc['Pre_GSHH_NDVI'].abs().max()\n",
    "print(f\"   Strongest color feature correlation: {max_color_corr:.3f}\")\n",
    "print(f\"   Strongest NDVI correlation: {ndvi_corr:.3f}\")\n",
    "\n",
    "if max_color_corr < 0.3:\n",
    "    red_flags.append(\"‚ö†Ô∏è  Very weak color-biomass correlations (r < 0.3)\")\n",
    "elif max_color_corr < 0.5:\n",
    "    print(\"   ‚ö†Ô∏è  Moderate color-biomass correlations (0.3 < r < 0.5)\")\n",
    "else:\n",
    "    print(\"   ‚úì Strong color-biomass correlations (r > 0.5)\")\n",
    "\n",
    "# Summary\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"RED FLAGS SUMMARY\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "if red_flags:\n",
    "    print(f\"\\nFound {len(red_flags)} potential issues:\\n\")\n",
    "    for flag in red_flags:\n",
    "        print(f\"  {flag}\")\n",
    "else:\n",
    "    print(\"\\n‚úì No major red flags detected\")\n",
    "    print(\"  Data quality appears acceptable for modeling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 9: Summary & Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"FINAL SUMMARY & RECOMMENDATIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nüìä FINDINGS:\")\n",
    "print(f\"\\n1. Simple Linear Models Performance:\")\n",
    "print(f\"   ‚Ä¢ NDVI only: {ndvi_score:.4f}\")\n",
    "print(f\"   ‚Ä¢ Color features: {color_score:.4f}\")\n",
    "print(f\"   ‚Ä¢ Simple RGB: {rgb_score:.4f}\")\n",
    "print(f\"   ‚Ä¢ Combined: {combined_score:.4f}\")\n",
    "\n",
    "print(f\"\\n2. Current CNN Performance:\")\n",
    "print(f\"   ‚Ä¢ Baseline CNN: -1.2527 (FAILED)\")\n",
    "print(f\"   ‚Ä¢ Teacher CNN: -2.1383 (FAILED)\")\n",
    "print(f\"   ‚Ä¢ Student CNN: -2.0922 (FAILED)\")\n",
    "\n",
    "print(f\"\\n3. Correlation Strengths:\")\n",
    "max_corr_per_target = {}\n",
    "for target in target_cols:\n",
    "    max_corr = target_correlations[target].abs().max()\n",
    "    max_feature = target_correlations[target].abs().idxmax()\n",
    "    max_corr_per_target[target] = (max_feature, max_corr)\n",
    "    print(f\"   ‚Ä¢ {target}: r={max_corr:.3f} (strongest: {max_feature})\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üîß RECOMMENDATIONS:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Decision logic\n",
    "if color_score > 0.2:\n",
    "    print(\"\\n‚úÖ PROCEED WITH CNN (with fixes):\")\n",
    "    print(\"\\n   Color features show promise (R¬≤ > 0.2).\")\n",
    "    print(\"   CNN should be able to learn these patterns.\")\n",
    "    print(\"\\n   Required fixes:\")\n",
    "    print(\"   1. ‚ùå REMOVE ColorJitter - it's destroying color signal\")\n",
    "    print(\"   2. üîß SIMPLIFY model architecture - current is too complex\")\n",
    "    print(\"   3. ‚è±Ô∏è  TRAIN for 5 epochs first - quick validation\")\n",
    "    print(\"   4. üéØ TARGET: Beat simple linear model (R¬≤ > {:.2f})\".format(color_score))\n",
    "    \n",
    "elif color_score > 0.0:\n",
    "    print(\"\\n‚ö†Ô∏è  CAUTIOUSLY PROCEED:\")\n",
    "    print(\"\\n   Color features show weak but positive correlation.\")\n",
    "    print(\"   CNN might work, but expectations should be modest.\")\n",
    "    print(\"\\n   Recommended actions:\")\n",
    "    print(\"   1. ‚ùå REMOVE ColorJitter\")\n",
    "    print(\"   2. üîß Try SIMPLER model first (fewer layers)\")\n",
    "    print(\"   3. üìä Consider ensemble with linear model\")\n",
    "    print(\"   4. üéØ TARGET: R¬≤ > 0.1 minimum\")\n",
    "    \n",
    "else:\n",
    "    print(\"\\n‚ùå STOP - DATA PROBLEM:\")\n",
    "    print(\"\\n   Even simple linear models can't predict biomass from color.\")\n",
    "    print(\"   This indicates fundamental data quality issues.\")\n",
    "    print(\"\\n   Required investigation:\")\n",
    "    print(\"   1. üîç Verify image-label alignment (correct IDs?)\")\n",
    "    print(\"   2. üìÖ Check date alignment (images from same day as biomass?)\")\n",
    "    print(\"   3. üñºÔ∏è  Manually inspect 20+ images vs labels\")\n",
    "    print(\"   4. üìä Investigate why color doesn't correlate with biomass\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"NEXT STEPS:\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\n1. Review all visualizations above\")\n",
    "print(\"2. Check if high-biomass images LOOK greener than low-biomass\")\n",
    "print(\"3. If yes ‚Üí Fix CNN training setup (remove ColorJitter, simplify)\")\n",
    "print(\"4. If no ‚Üí Investigate data quality issues\")\n",
    "print(\"5. Train simple baseline for 5 epochs only\")\n",
    "print(\"6. If R¬≤ > 0.0 ‚Üí Scale up training\")\n",
    "print(\"7. If R¬≤ < 0.0 ‚Üí Debug further before long training runs\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úì Feature exploration complete!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the enriched data with color features for later use\n",
    "data_with_features.to_csv('train_with_color_features.csv', index=False)\n",
    "print(\"\\n‚úì Saved enriched data with color features to: train_with_color_features.csv\")\n",
    "print(\"  You can use this for further analysis or quick experiments\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
