{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSIRO Biomass Prediction - Universal Features K-Fold Ensemble\n",
    "\n",
    "## Model: 5-Fold CV with Universal Features + Species\n",
    "\n",
    "### Why This Approach?\n",
    "\n",
    "**Previous K-Fold ensemble** scored **0.50** on Kaggle (worse than baseline 0.51!).\n",
    "\n",
    "**Key insight from 22nd place leaderboard user:**\n",
    "> \"Test dataset uses locations that aren't in the training dataset\"\n",
    "\n",
    "### The Problem with Previous Approach\n",
    "\n",
    "‚ùå **State classification** (NSW/Tas/Vic/WA) ‚Üí Useless for unseen test locations!  \n",
    "‚ùå **Weather features** (rainfall, temp, ET0) ‚Üí Location-specific climate patterns  \n",
    "\n",
    "Models learned to recognize **training locations**, not general biomass patterns.\n",
    "\n",
    "### New Strategy: Universal Features + Species\n",
    "\n",
    "‚úÖ **NDVI** - Vegetation density (universal)  \n",
    "‚úÖ **Height** - Plant height (universal)  \n",
    "‚úÖ **Season/Daylength** - Calendar-based (universal)  \n",
    "‚úÖ **Species** - Plant type (universal! Ryegrass biomass similar everywhere)  \n",
    "\n",
    "‚ùå **Removed:** State, Weather (location-specific)\n",
    "\n",
    "### Expected Improvement\n",
    "\n",
    "- **Previous K-Fold**: 0.50 (learned location bias)\n",
    "- **This approach**: **0.52-0.54** (should generalize to new locations!)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Setup & Imports\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "\n",
    "print(\"‚úì Setup complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Configuration\n",
    "\n",
    "# Target columns (order matters!)\n",
    "TARGET_COLS = ['Dry_Green_g', 'Dry_Dead_g', 'Dry_Clover_g', 'GDM_g', 'Dry_Total_g']\n",
    "\n",
    "# Target normalization statistics (calculated from FULL training set - 357 images)\n",
    "# These stats are CONSISTENT across all 5 folds\n",
    "TARGET_MEANS = torch.tensor([\n",
    "    26.624722,  # Dry_Green_g\n",
    "    12.044548,  # Dry_Dead_g\n",
    "    6.649692,   # Dry_Clover_g\n",
    "    33.274414,  # GDM_g\n",
    "    45.318097   # Dry_Total_g\n",
    "], dtype=torch.float32)\n",
    "\n",
    "TARGET_STDS = torch.tensor([\n",
    "    25.401232,  # Dry_Green_g\n",
    "    12.402007,  # Dry_Dead_g\n",
    "    12.117761,  # Dry_Clover_g\n",
    "    24.935822,  # GDM_g\n",
    "    27.984015   # Dry_Total_g\n",
    "], dtype=torch.float32)\n",
    "\n",
    "# Inference batch size\n",
    "BATCH_SIZE = 16\n",
    "NUM_FOLDS = 5\n",
    "NUM_SPECIES = 15  # Unique plant species\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Model: 5-Fold CV Ensemble (Universal Features + Species)\")\n",
    "print(f\"  Number of models: {NUM_FOLDS}\")\n",
    "print(f\"  Auxiliary tasks: NDVI, Height, Daylength, Season, Species (15 classes)\")\n",
    "print(f\"  Removed: State, Weather (location-specific!)\")\n",
    "print(f\"  Targets: {TARGET_COLS}\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"\\n‚úì Configuration loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Model Architecture\n",
    "\n",
    "class UniversalAuxiliaryModel(nn.Module):\n",
    "    \"\"\"ResNet18 with UNIVERSAL auxiliary tasks + Species classification.\n",
    "    \n",
    "    Training approach:\n",
    "    - Phase 1: Auxiliary pretraining (predict NDVI, height, daylength, season, species from images)\n",
    "    - Phase 2: Biomass fine-tuning (predict 5 biomass targets)\n",
    "    \n",
    "    Key difference from previous: Removed State and Weather (location-specific)\n",
    "    Why Species is kept: Same species has similar biomass anywhere (universal!)\n",
    "    \n",
    "    This model was trained 5 times with different train/val splits (K-Fold CV).\n",
    "    \"\"\"\n",
    "    def __init__(self, num_outputs=5, hidden_dim=256, dropout=0.2, num_species=15):\n",
    "        super().__init__()\n",
    "        # ResNet18 backbone (weights=None means no pretrained ImageNet weights)\n",
    "        model = models.resnet18(weights=None)  # No download needed!\n",
    "        self.backbone = nn.Sequential(*list(model.children())[:-1])\n",
    "        feature_dim = 512\n",
    "        \n",
    "        # UNIVERSAL auxiliary heads (required for loading checkpoint, not used in inference)\n",
    "        self.ndvi_head = nn.Linear(feature_dim, 1)\n",
    "        self.height_head = nn.Linear(feature_dim, 1)\n",
    "        self.daylength_head = nn.Linear(feature_dim, 1)\n",
    "        self.season_head = nn.Linear(feature_dim, 1)\n",
    "        self.species_head = nn.Linear(feature_dim, num_species)  # 15 species classes\n",
    "        \n",
    "        # NO State/Weather heads! (location-specific, removed)\n",
    "        \n",
    "        # Biomass head (this is what we use for predictions)\n",
    "        self.biomass_head = nn.Sequential(\n",
    "            nn.Linear(feature_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, num_outputs)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, mode='biomass'):\n",
    "        features = self.backbone(x).flatten(1)\n",
    "        if mode == 'auxiliary':\n",
    "            return {\n",
    "                'ndvi': self.ndvi_head(features),\n",
    "                'height': self.height_head(features),\n",
    "                'daylength': self.daylength_head(features),\n",
    "                'season': self.season_head(features),\n",
    "                'species': self.species_head(features)\n",
    "            }\n",
    "        else:\n",
    "            return self.biomass_head(features)\n",
    "\n",
    "print(\"‚úì UniversalAuxiliaryModel defined\")\n",
    "print(\"  5 auxiliary heads: NDVI, Height, Daylength, Season, Species\")\n",
    "print(\"  Removed: State (4 classes), Weather (14 features)\")\n",
    "print(\"  Why: Test locations not in training ‚Üí State/Weather don't generalize!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Load All 5 Fold Model Checkpoints\n",
    "\n",
    "import os\n",
    "\n",
    "print(f\"Loading {NUM_FOLDS} fold models...\\n\")\n",
    "\n",
    "# Get current working directory for better path resolution\n",
    "cwd = os.getcwd()\n",
    "print(f\"Current working directory: {cwd}\\n\")\n",
    "\n",
    "fold_models = []\n",
    "checkpoint_loaded_count = 0\n",
    "\n",
    "for fold_idx in range(1, NUM_FOLDS + 1):\n",
    "    checkpoint_name = f'universal_Fold{fold_idx}_best.pth'\n",
    "    \n",
    "    # Try multiple checkpoint paths (local testing vs Kaggle submission)\n",
    "    checkpoint_paths = [\n",
    "        f'./{checkpoint_name}',  # Local path (same directory)\n",
    "        checkpoint_name,  # Try without ./\n",
    "        os.path.join(cwd, checkpoint_name),  # Absolute path\n",
    "        f'../input/csiro-biomass-universal-kfold/{checkpoint_name}',  # Kaggle input\n",
    "        f'/kaggle/input/csiro-biomass-universal-kfold/{checkpoint_name}',  # Alternative Kaggle path\n",
    "    ]\n",
    "    \n",
    "    model = UniversalAuxiliaryModel(num_species=NUM_SPECIES)\n",
    "    checkpoint_loaded = False\n",
    "    \n",
    "    for path in checkpoint_paths:\n",
    "        if Path(path).exists():\n",
    "            print(f\"Fold {fold_idx}: Found at {path}\")\n",
    "            model.load_state_dict(torch.load(path, map_location=device))\n",
    "            model = model.to(device)\n",
    "            model.eval()\n",
    "            fold_models.append(model)\n",
    "            checkpoint_loaded = True\n",
    "            checkpoint_loaded_count += 1\n",
    "            break\n",
    "    \n",
    "    if not checkpoint_loaded:\n",
    "        print(f\"\\n‚ùå Could not find checkpoint for Fold {fold_idx}!\\n\")\n",
    "        print(f\"Tried paths:\")\n",
    "        for p in checkpoint_paths:\n",
    "            exists = \"‚úì\" if Path(p).exists() else \"‚úó\"\n",
    "            print(f\"  {exists} {p}\")\n",
    "\n",
    "if checkpoint_loaded_count != NUM_FOLDS:\n",
    "    print(f\"\\n\" + \"=\"*80)\n",
    "    print(\"FOR KAGGLE SUBMISSION:\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"1. Upload all 5 model checkpoints as a Kaggle Dataset:\")\n",
    "    print(\"   - universal_Fold1_best.pth\")\n",
    "    print(\"   - universal_Fold2_best.pth\")\n",
    "    print(\"   - universal_Fold3_best.pth\")\n",
    "    print(\"   - universal_Fold4_best.pth\")\n",
    "    print(\"   - universal_Fold5_best.pth\")\n",
    "    print(\"2. Add the dataset as input to this notebook (click 'Add Data' button)\")\n",
    "    print(\"3. Update the checkpoint_paths list above with your dataset name\")\n",
    "    print(\"   Example: '../input/YOUR-DATASET-NAME/{checkpoint_name}'\")\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"FOR LOCAL TESTING:\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Ensure all 5 checkpoint files are in: {cwd}\")\n",
    "    print(\"=\"*80)\n",
    "    raise FileNotFoundError(f\"Only loaded {checkpoint_loaded_count}/{NUM_FOLDS} model checkpoints\")\n",
    "\n",
    "print(f\"\\n‚úÖ Successfully loaded all {len(fold_models)} fold models!\")\n",
    "print(f\"  Device: {device}\")\n",
    "print(f\"  Mode: Inference (eval mode)\")\n",
    "print(f\"  Parameters per model: {sum(p.numel() for p in fold_models[0].parameters()):,}\")\n",
    "print(f\"  Total ensemble parameters: {sum(p.numel() for p in fold_models[0].parameters()) * NUM_FOLDS:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 5: Load Test Data\n\nprint(\"Loading test data...\\n\")\n\n# Try multiple test data paths (local testing vs Kaggle submission)\ntest_csv_paths = [\n    './competition/test.csv',  # Local testing path\n    '/kaggle/input/csiro-biomass/test.csv',  # Correct Kaggle path\n    '../input/csiro-biomass/test.csv',  # Alternative Kaggle format\n]\n\ntest_df = None\nfor path in test_csv_paths:\n    if Path(path).exists():\n        print(f\"Found test.csv at: {path}\")\n        test_df = pd.read_csv(path)\n        base_path = str(Path(path).parent)\n        break\n\nif test_df is None:\n    raise FileNotFoundError(\"Could not find test.csv\")\n\nprint(f\"\\nTest data shape: {test_df.shape}\")\nprint(f\"Columns: {list(test_df.columns)}\")\nprint(f\"\\nFirst few rows:\")\nprint(test_df.head())\n\n# Extract unique images from long format\ntest_df['full_image_path'] = test_df['image_path'].apply(lambda x: f\"{base_path}/{x}\")\nunique_images_df = test_df[['image_path', 'full_image_path']].drop_duplicates().reset_index(drop=True)\n\nprint(f\"\\n‚úì Found {len(unique_images_df)} unique test images\")\nprint(f\"  Total test rows: {len(test_df)} (images √ó targets)\")\nprint(f\"  Expected: {len(unique_images_df)} images √ó 5 targets = {len(unique_images_df) * 5} rows\")\n\n# Verify all images exist\nmissing_images = []\nfor path in unique_images_df['full_image_path']:\n    if not Path(path).exists():\n        missing_images.append(path)\n\nif missing_images:\n    print(f\"\\n‚ö†Ô∏è  WARNING: {len(missing_images)} images not found:\")\n    for img in missing_images[:5]:\n        print(f\"  - {img}\")\n    if len(missing_images) > 5:\n        print(f\"  ... and {len(missing_images) - 5} more\")\nelse:\n    print(f\"\\n‚úì All {len(unique_images_df)} test images found!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Create Test Dataset & DataLoader\n",
    "\n",
    "class TestDataset(Dataset):\n",
    "    \"\"\"Test dataset for inference (images only, no labels).\"\"\"\n",
    "    \n",
    "    def __init__(self, image_paths):\n",
    "        self.image_paths = image_paths\n",
    "        \n",
    "        # Same transforms used during training (without augmentation)\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # ImageNet stats\n",
    "        ])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        img = self.transform(img)\n",
    "        return img\n",
    "\n",
    "# Create dataset and dataloader\n",
    "test_dataset = TestDataset(unique_images_df['full_image_path'].tolist())\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=False,  # Important: Keep order for matching predictions to images\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "print(f\"‚úì Test dataset created\")\n",
    "print(f\"  Images: {len(test_dataset)}\")\n",
    "print(f\"  Batches: {len(test_loader)}\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Generate Ensemble Predictions\n",
    "\n",
    "print(f\"Generating predictions from {NUM_FOLDS} models...\\n\")\n",
    "\n",
    "# Get predictions from each fold model\n",
    "all_fold_predictions = []\n",
    "\n",
    "for fold_idx, model in enumerate(fold_models, 1):\n",
    "    print(f\"Fold {fold_idx}/{NUM_FOLDS}...\")\n",
    "    fold_preds = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images in tqdm(test_loader, desc=f\"  Predicting\", leave=False):\n",
    "            images = images.to(device)\n",
    "            \n",
    "            # Forward pass (returns normalized predictions)\n",
    "            outputs = model(images, mode='biomass')  # [batch_size, 5]\n",
    "            \n",
    "            # Denormalize to original scale (grams)\n",
    "            outputs_denorm = outputs.cpu() * TARGET_STDS + TARGET_MEANS\n",
    "            \n",
    "            # Clip negative values to 0 (biomass cannot be negative)\n",
    "            outputs_denorm = torch.clamp(outputs_denorm, min=0)\n",
    "            \n",
    "            fold_preds.append(outputs_denorm.numpy())\n",
    "    \n",
    "    fold_predictions = np.vstack(fold_preds)  # [num_images, 5]\n",
    "    all_fold_predictions.append(fold_predictions)\n",
    "    print(f\"  ‚úì Shape: {fold_predictions.shape}\")\n",
    "\n",
    "# Ensemble: Average predictions across all folds\n",
    "all_predictions = np.mean(all_fold_predictions, axis=0)  # [num_images, 5]\n",
    "\n",
    "print(f\"\\n‚úÖ Ensemble predictions generated!\")\n",
    "print(f\"  Shape: {all_predictions.shape} (images √ó targets)\")\n",
    "print(f\"  Method: Averaged across {NUM_FOLDS} models\")\n",
    "print(f\"\\nPrediction statistics (grams):\")\n",
    "for i, col in enumerate(TARGET_COLS):\n",
    "    print(f\"  {col:15s}: min={all_predictions[:, i].min():7.2f}g, \"\n",
    "          f\"max={all_predictions[:, i].max():7.2f}g, \"\n",
    "          f\"mean={all_predictions[:, i].mean():7.2f}g\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Create Submission File\n",
    "\n",
    "import os\n",
    "\n",
    "print(\"Creating submission file...\\n\")\n",
    "\n",
    "# Convert predictions to long format (one row per sample_id)\n",
    "submission_rows = []\n",
    "\n",
    "for idx, img_path in enumerate(unique_images_df['image_path'].tolist()):\n",
    "    # Extract image ID from path (e.g., 'test/ID1001187975.jpg' -> 'ID1001187975')\n",
    "    image_id = Path(img_path).stem  # Get filename without extension\n",
    "    \n",
    "    # Create one row per target (5 rows per image)\n",
    "    for target_idx, target_name in enumerate(TARGET_COLS):\n",
    "        sample_id = f\"{image_id}__{target_name}\"  # Format: ImageID__TargetName\n",
    "        target_value = all_predictions[idx, target_idx]\n",
    "        \n",
    "        submission_rows.append({\n",
    "            'sample_id': sample_id,\n",
    "            'target': target_value\n",
    "        })\n",
    "\n",
    "# Create DataFrame\n",
    "submission = pd.DataFrame(submission_rows)\n",
    "\n",
    "print(\"Submission DataFrame:\")\n",
    "print(submission.head(10))\n",
    "print(f\"\\nShape: {submission.shape}\")\n",
    "print(f\"Expected: ({len(unique_images_df) * 5}, 2)\")\n",
    "\n",
    "# Quality checks\n",
    "print(f\"\\nQuality checks:\")\n",
    "print(f\"  NaN values: {submission.isna().sum().sum()} ‚úì\" if submission.isna().sum().sum() == 0 else f\"  ‚ö†Ô∏è  NaN values: {submission.isna().sum().sum()}\")\n",
    "print(f\"  Infinite values: {np.isinf(submission['target']).sum()} ‚úì\" if np.isinf(submission['target']).sum() == 0 else f\"  ‚ö†Ô∏è  Infinite values: {np.isinf(submission['target']).sum()}\")\n",
    "print(f\"  Negative values: {(submission['target'] < 0).sum()} ‚úì\" if (submission['target'] < 0).sum() == 0 else f\"  ‚ö†Ô∏è  Negative values: {(submission['target'] < 0).sum()}\")\n",
    "print(f\"  Correct columns: {list(submission.columns) == ['sample_id', 'target']} ‚úì\" if list(submission.columns) == ['sample_id', 'target'] else f\"  ‚ö†Ô∏è  Columns: {list(submission.columns)}\")\n",
    "\n",
    "# IMPORTANT: Save to current working directory for Kaggle compatibility\n",
    "output_path = 'submission.csv'\n",
    "submission.to_csv(output_path, index=False)\n",
    "\n",
    "# Verify file was created\n",
    "if os.path.exists(output_path):\n",
    "    file_size = os.path.getsize(output_path)\n",
    "    print(f\"\\n‚úÖ File verified: {output_path} ({file_size:,} bytes)\")\n",
    "else:\n",
    "    raise FileNotFoundError(f\"Failed to create {output_path}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"‚úÖ SUBMISSION FILE CREATED: submission.csv\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"\\nFile details:\")\n",
    "print(f\"  Filename: submission.csv (required by Kaggle)\")\n",
    "print(f\"  Location: {os.path.abspath(output_path)}\")\n",
    "print(f\"  Rows: {len(submission):,}\")\n",
    "print(f\"  Images: {len(unique_images_df)}\")\n",
    "print(f\"  Format: Long format (sample_id, target)\")\n",
    "print(f\"\\nModel info:\")\n",
    "print(f\"  Approach: 5-Fold CV with Universal Features + Species\")\n",
    "print(f\"  Architecture: ResNet18 + Auxiliary Pretraining\")\n",
    "print(f\"  Auxiliary tasks: NDVI, Height, Daylength, Season, Species (15 classes)\")\n",
    "print(f\"  Removed: State, Weather (location-specific!)\")\n",
    "print(f\"\\nExpected Kaggle score: 0.52-0.54\")\n",
    "print(f\"  (Previous K-Fold: 0.50 - learned location bias)\")\n",
    "print(f\"  (Baseline: 0.51)\")\n",
    "print(f\"\\nWhy this should work better:\")\n",
    "print(f\"  1. Universal features generalize to new locations\")\n",
    "print(f\"  2. Species is universal (Ryegrass biomass similar everywhere)\")\n",
    "print(f\"  3. No State/Weather ‚Üí no location bias\")\n",
    "print(f\"  4. Ensemble averaging reduces overfitting\")\n",
    "print(f\"\\nNext steps:\")\n",
    "print(f\"  1. Download submission.csv from notebook output\")\n",
    "print(f\"  2. Submit to Kaggle competition\")\n",
    "print(f\"  3. Compare with previous K-Fold (0.50) and baseline (0.51)\")\n",
    "print(f\"  4. Expected improvement: +0.01 to +0.03\")\n",
    "print(f\"\\n{'='*80}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Final Verification (For Kaggle)\n",
    "\n",
    "import os\n",
    "import glob\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL VERIFICATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# List all CSV files in current directory\n",
    "csv_files = glob.glob('*.csv')\n",
    "print(f\"\\nCSV files in current directory:\")\n",
    "for f in csv_files:\n",
    "    size = os.path.getsize(f)\n",
    "    print(f\"  {f}: {size:,} bytes\")\n",
    "\n",
    "# Verify submission.csv specifically\n",
    "if os.path.exists('submission.csv'):\n",
    "    size = os.path.getsize('submission.csv')\n",
    "    print(f\"\\n‚úÖ SUCCESS! submission.csv exists ({size:,} bytes)\")\n",
    "    print(f\"   Absolute path: {os.path.abspath('submission.csv')}\")\n",
    "    \n",
    "    # Show first few lines\n",
    "    import pandas as pd\n",
    "    sub = pd.read_csv('submission.csv')\n",
    "    print(f\"\\nFirst 10 rows:\")\n",
    "    print(sub.head(10))\n",
    "    print(f\"\\nTotal rows: {len(sub)}\")\n",
    "    print(f\"Columns: {list(sub.columns)}\")\n",
    "    print(f\"\\nTarget statistics:\")\n",
    "    print(f\"  Min: {sub['target'].min():.2f}g\")\n",
    "    print(f\"  Max: {sub['target'].max():.2f}g\")\n",
    "    print(f\"  Mean: {sub['target'].mean():.2f}g\")\n",
    "    print(f\"  Median: {sub['target'].median():.2f}g\")\n",
    "else:\n",
    "    print(f\"\\n‚ùå ERROR! submission.csv not found!\")\n",
    "    print(f\"Current directory: {os.getcwd()}\")\n",
    "    print(f\"Files in directory: {os.listdir('.')}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üéØ READY FOR KAGGLE SUBMISSION!\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nKey improvements over previous K-Fold (0.50):\")\n",
    "print(\"  ‚úÖ Removed State classification (location-specific)\")\n",
    "print(\"  ‚úÖ Removed Weather features (location-specific)\")\n",
    "print(\"  ‚úÖ Kept Species (universal - same species similar everywhere!)\")\n",
    "print(\"  ‚úÖ Kept NDVI, Height, Season, Daylength (universal)\")\n",
    "print(\"\\nExpected: 0.52-0.54 (should generalize to new test locations!)\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}