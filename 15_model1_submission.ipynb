{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CSIRO Biomass Prediction - Model 1 (Simple Baseline) Submission\n",
        "\n",
        "## Model: 1 - Simple ResNet18 Baseline\n",
        "\n",
        "**Validation R\u00b2**: +0.6352 (10 epochs)\n",
        "\n",
        "### Why Test This Model?\n",
        "\n",
        "Model 4b (auxiliary pretrained) achieved **R\u00b2=+0.6852** on validation but only **R\u00b2=+0.51** on Kaggle test set.\n",
        "\n",
        "**This is a -0.175 gap!** Possible causes:\n",
        "1. **Overfitting** - Model 4b trained for 30 epochs, may have memorized training data\n",
        "2. **Complexity** - More complex models often overfit on small datasets  \n",
        "3. **Wrong normalization** - Used split stats instead of full dataset stats\n",
        "\n",
        "### Model 1 Advantages\n",
        "\n",
        "\u2705 **Simpler architecture** - Plain ResNet18 with FC head (no auxiliary heads)\n",
        "\u2705 **Less training** - Only 10 epochs (vs 30 for Model 4b)\n",
        "\u2705 **Less overfit** - Validation R\u00b2 increased steadily without bouncing\n",
        "\u2705 **No ColorJitter** - Avoided harmful augmentation\n",
        "\n",
        "**Expected Kaggle score**: 0.55-0.60 (better generalization than Model 4b)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n",
            "\u2713 Setup complete\n"
          ]
        }
      ],
      "source": [
        "# Cell 1: Setup & Imports\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# Setup\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Set seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(42)\n",
        "\n",
        "print(\"\u2713 Setup complete\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Configuration:\n",
            "  Model: Simple ResNet18 Baseline\n",
            "  Validation R\u00b2: +0.6352\n",
            "  Training: 10 epochs\n",
            "  Targets: ['Dry_Green_g', 'Dry_Dead_g', 'Dry_Clover_g', 'GDM_g', 'Dry_Total_g']\n",
            "  Batch size: 16\n",
            "\n",
            "\u2713 Configuration loaded\n"
          ]
        }
      ],
      "source": [
        "# Cell 2: Configuration\n",
        "\n",
        "# Target columns (order matters!)\n",
        "TARGET_COLS = ['Dry_Green_g', 'Dry_Dead_g', 'Dry_Clover_g', 'GDM_g', 'Dry_Total_g']\n",
        "\n",
        "# Target normalization statistics (calculated from FULL training set - 357 images)\n",
        "# IMPORTANT: Model 1 was trained with SPLIT stats, but we should use FULL dataset stats\n",
        "# for consistency with Kaggle expectations\n",
        "TARGET_MEANS = torch.tensor([\n",
        "    26.624722,  # Dry_Green_g\n",
        "    12.044548,  # Dry_Dead_g\n",
        "    6.649692,   # Dry_Clover_g\n",
        "    33.274414,  # GDM_g\n",
        "    45.318097   # Dry_Total_g\n",
        "], dtype=torch.float32)\n",
        "\n",
        "TARGET_STDS = torch.tensor([\n",
        "    25.401232,  # Dry_Green_g\n",
        "    12.402007,  # Dry_Dead_g\n",
        "    12.117761,  # Dry_Clover_g\n",
        "    24.935822,  # GDM_g\n",
        "    27.984015   # Dry_Total_g\n",
        "], dtype=torch.float32)\n",
        "\n",
        "# Inference batch size\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "print(\"Configuration:\")\n",
        "print(f\"  Model: Simple ResNet18 Baseline\")\n",
        "print(f\"  Validation R\u00b2: +0.6352\")\n",
        "print(f\"  Training: 10 epochs\")\n",
        "print(f\"  Targets: {TARGET_COLS}\")\n",
        "print(f\"  Batch size: {BATCH_SIZE}\")\n",
        "print(\"\\n\u2713 Configuration loaded\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u2713 SimpleBaseline model defined\n"
          ]
        }
      ],
      "source": [
        "# Cell 3: Model Architecture\n",
        "\n",
        "class SimpleBaseline(nn.Module):\n",
        "    \"\"\"Simple ResNet18 baseline for normalized targets.\n",
        "    \n",
        "    Architecture:\n",
        "    - ResNet18 backbone (pretrained on ImageNet)\n",
        "    - 512 \u2192 256 \u2192 ReLU \u2192 Dropout(0.2) \u2192 256 \u2192 5\n",
        "    \n",
        "    Total parameters: ~11.2M\n",
        "    \"\"\"\n",
        "    def __init__(self, num_outputs=5):\n",
        "        super().__init__()\n",
        "        self.resnet = models.resnet18(pretrained=False)  # We'll load trained weights\n",
        "        num_features = self.resnet.fc.in_features  # 512\n",
        "        \n",
        "        self.resnet.fc = nn.Sequential(\n",
        "            nn.Linear(num_features, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(256, num_outputs)\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.resnet(x)\n",
        "\n",
        "print(\"\u2713 SimpleBaseline model defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading Model 1 checkpoint...\n",
            "\n",
            "Current working directory: /Users/tim/Code/Tim/csiro-biomass\n",
            "Found checkpoint at: ./Model_1_Simple_best.pth\n",
            "\n",
            "\u2713 Model loaded successfully!\n",
            "  Device: cpu\n",
            "  Mode: Inference (eval mode)\n",
            "  Parameters: 11,309,125\n"
          ]
        }
      ],
      "source": [
        "# Cell 4: Load Model Checkpoint\n",
        "\n",
        "import os\n",
        "\n",
        "print(\"Loading Model 1 checkpoint...\\n\")\n",
        "\n",
        "# Create model instance\n",
        "model = SimpleBaseline(num_outputs=5)\n",
        "\n",
        "# Get current working directory for better path resolution\n",
        "cwd = os.getcwd()\n",
        "print(f\"Current working directory: {cwd}\")\n",
        "\n",
        "# Try multiple checkpoint paths (local testing vs Kaggle submission)\n",
        "checkpoint_paths = [\n",
        "    './Model_1_Simple_best.pth',  # Local path (same directory)\n",
        "    'Model_1_Simple_best.pth',    # Try without ./\n",
        "    os.path.join(cwd, 'Model_1_Simple_best.pth'),  # Absolute path\n",
        "    '../input/csiro-biomass-model1-weights/Model_1_Simple_best.pth',  # Kaggle input\n",
        "    '/kaggle/input/csiro-biomass-model1-weights/Model_1_Simple_best.pth',  # Alternative Kaggle path\n",
        "]\n",
        "\n",
        "checkpoint_loaded = False\n",
        "for path in checkpoint_paths:\n",
        "    if Path(path).exists():\n",
        "        print(f\"Found checkpoint at: {path}\")\n",
        "        model.load_state_dict(torch.load(path, map_location=device))\n",
        "        checkpoint_loaded = True\n",
        "        break\n",
        "\n",
        "if not checkpoint_loaded:\n",
        "    print(\"\\n\u274c Could not find model checkpoint!\\n\")\n",
        "    print(\"Tried paths:\")\n",
        "    for p in checkpoint_paths:\n",
        "        exists = \"\u2713\" if Path(p).exists() else \"\u2717\"\n",
        "        print(f\"  {exists} {p}\")\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"FOR KAGGLE SUBMISSION:\")\n",
        "    print(\"=\"*80)\n",
        "    print(\"1. Upload 'Model_1_Simple_best.pth' as a Kaggle Dataset\")\n",
        "    print(\"2. Add the dataset as input to this notebook (click 'Add Data' button)\")\n",
        "    print(\"3. Update the checkpoint_paths list above with your dataset name\")\n",
        "    print(\"   Example: '../input/YOUR-DATASET-NAME/Model_1_Simple_best.pth'\")\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"FOR LOCAL TESTING:\")\n",
        "    print(\"=\"*80)\n",
        "    print(\"Ensure 'Model_1_Simple_best.pth' is in:\")\n",
        "    print(f\"  {cwd}\")\n",
        "    print(\"=\"*80)\n",
        "    raise FileNotFoundError(\"Model checkpoint not found\")\n",
        "\n",
        "# Prepare model for inference\n",
        "model = model.to(device)\n",
        "model.eval()\n",
        "\n",
        "print(f\"\\n\u2713 Model loaded successfully!\")\n",
        "print(f\"  Device: {device}\")\n",
        "print(f\"  Mode: Inference (eval mode)\")\n",
        "print(f\"  Parameters: {sum(p.numel() for p in model.parameters()):,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading test data...\n",
            "\n",
            "Found test.csv at: ./competition/test.csv\n",
            "\n",
            "Test data shape: (5, 3)\n",
            "Columns: ['sample_id', 'image_path', 'target_name']\n",
            "\n",
            "First few rows:\n",
            "                    sample_id             image_path   target_name\n",
            "0  ID1001187975__Dry_Clover_g  test/ID1001187975.jpg  Dry_Clover_g\n",
            "1    ID1001187975__Dry_Dead_g  test/ID1001187975.jpg    Dry_Dead_g\n",
            "2   ID1001187975__Dry_Green_g  test/ID1001187975.jpg   Dry_Green_g\n",
            "3   ID1001187975__Dry_Total_g  test/ID1001187975.jpg   Dry_Total_g\n",
            "4         ID1001187975__GDM_g  test/ID1001187975.jpg         GDM_g\n",
            "\n",
            "\u2713 Found 1 unique test images\n",
            "  Total test rows: 5 (images \u00d7 targets)\n",
            "  Expected: 1 images \u00d7 5 targets = 5 rows\n",
            "\n",
            "\u2713 All 1 test images found!\n"
          ]
        }
      ],
      "source": [
        "# Cell 5: Load Test Data\n",
        "\n",
        "print(\"Loading test data...\\n\")\n",
        "\n",
        "# Try multiple test data paths (local testing vs Kaggle submission)\n",
        "test_csv_paths = [\n",
        "    './competition/test.csv',  # Local path\n",
        "    '../input/csiro-biomass-prediction/test.csv',  # Kaggle input path (typical)\n",
        "    '/kaggle/input/csiro-biomass-prediction/test.csv',  # Alternative Kaggle path\n",
        "]\n",
        "\n",
        "test_df = None\n",
        "for path in test_csv_paths:\n",
        "    if Path(path).exists():\n",
        "        print(f\"Found test.csv at: {path}\")\n",
        "        test_df = pd.read_csv(path)\n",
        "        base_path = str(Path(path).parent)\n",
        "        break\n",
        "\n",
        "if test_df is None:\n",
        "    raise FileNotFoundError(\"Could not find test.csv\")\n",
        "\n",
        "print(f\"\\nTest data shape: {test_df.shape}\")\n",
        "print(f\"Columns: {list(test_df.columns)}\")\n",
        "print(f\"\\nFirst few rows:\")\n",
        "print(test_df.head())\n",
        "\n",
        "# Extract unique images from long format\n",
        "# test.csv format: sample_id, image_path, target_name (one row per image\u00d7target combination)\n",
        "test_df['full_image_path'] = test_df['image_path'].apply(lambda x: f\"{base_path}/{x}\")\n",
        "unique_images_df = test_df[['image_path', 'full_image_path']].drop_duplicates().reset_index(drop=True)\n",
        "\n",
        "print(f\"\\n\u2713 Found {len(unique_images_df)} unique test images\")\n",
        "print(f\"  Total test rows: {len(test_df)} (images \u00d7 targets)\")\n",
        "print(f\"  Expected: {len(unique_images_df)} images \u00d7 5 targets = {len(unique_images_df) * 5} rows\")\n",
        "\n",
        "# Verify all images exist\n",
        "missing_images = []\n",
        "for path in unique_images_df['full_image_path']:\n",
        "    if not Path(path).exists():\n",
        "        missing_images.append(path)\n",
        "\n",
        "if missing_images:\n",
        "    print(f\"\\n\u26a0\ufe0f  WARNING: {len(missing_images)} images not found:\")\n",
        "    for img in missing_images[:5]:\n",
        "        print(f\"  - {img}\")\n",
        "    if len(missing_images) > 5:\n",
        "        print(f\"  ... and {len(missing_images) - 5} more\")\n",
        "else:\n",
        "    print(f\"\\n\u2713 All {len(unique_images_df)} test images found!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u2713 Test dataset created\n",
            "  Images: 1\n",
            "  Batches: 1\n",
            "  Batch size: 16\n"
          ]
        }
      ],
      "source": [
        "# Cell 6: Create Test Dataset & DataLoader\n",
        "\n",
        "class TestDataset(Dataset):\n",
        "    \"\"\"Test dataset for inference (images only, no labels).\"\"\"\n",
        "    \n",
        "    def __init__(self, image_paths):\n",
        "        self.image_paths = image_paths\n",
        "        \n",
        "        # Same transforms used during training (without augmentation)\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Resize((224, 224)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # ImageNet stats\n",
        "        ])\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_paths[idx]\n",
        "        img = Image.open(img_path).convert('RGB')\n",
        "        img = self.transform(img)\n",
        "        return img\n",
        "\n",
        "# Create dataset and dataloader\n",
        "test_dataset = TestDataset(unique_images_df['full_image_path'].tolist())\n",
        "test_loader = DataLoader(\n",
        "    test_dataset, \n",
        "    batch_size=BATCH_SIZE, \n",
        "    shuffle=False,  # Important: Keep order for matching predictions to images\n",
        "    num_workers=0\n",
        ")\n",
        "\n",
        "print(f\"\u2713 Test dataset created\")\n",
        "print(f\"  Images: {len(test_dataset)}\")\n",
        "print(f\"  Batches: {len(test_loader)}\")\n",
        "print(f\"  Batch size: {BATCH_SIZE}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating predictions...\n",
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "70ccb0bb357c46718b67e92122c489c6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Predicting:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u2713 Predictions generated!\n",
            "  Shape: (1, 5) (images \u00d7 targets)\n",
            "\n",
            "Prediction statistics (grams):\n",
            "  Dry_Green_g    : min=  15.72g, max=  15.72g, mean=  15.72g\n",
            "  Dry_Dead_g     : min=  19.35g, max=  19.35g, mean=  19.35g\n",
            "  Dry_Clover_g   : min=   1.30g, max=   1.30g, mean=   1.30g\n",
            "  GDM_g          : min=  19.12g, max=  19.12g, mean=  19.12g\n",
            "  Dry_Total_g    : min=  34.26g, max=  34.26g, mean=  34.26g\n"
          ]
        }
      ],
      "source": [
        "# Cell 7: Generate Predictions\n",
        "\n",
        "print(\"Generating predictions...\\n\")\n",
        "\n",
        "all_predictions = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch_idx, images in enumerate(tqdm(test_loader, desc='Predicting')):\n",
        "        images = images.to(device)\n",
        "        \n",
        "        # Forward pass (returns normalized predictions)\n",
        "        outputs = model(images)  # [batch_size, 5]\n",
        "        \n",
        "        # Denormalize to original scale (grams)\n",
        "        outputs_denorm = outputs.cpu() * TARGET_STDS + TARGET_MEANS\n",
        "        \n",
        "        # Clip negative values to 0 (biomass cannot be negative)\n",
        "        outputs_denorm = torch.clamp(outputs_denorm, min=0)\n",
        "        \n",
        "        all_predictions.append(outputs_denorm.numpy())\n",
        "\n",
        "# Stack all predictions\n",
        "all_predictions = np.vstack(all_predictions)  # [num_images, 5]\n",
        "\n",
        "print(f\"\\n\u2713 Predictions generated!\")\n",
        "print(f\"  Shape: {all_predictions.shape} (images \u00d7 targets)\")\n",
        "print(f\"\\nPrediction statistics (grams):\")\n",
        "for i, col in enumerate(TARGET_COLS):\n",
        "    print(f\"  {col:15s}: min={all_predictions[:, i].min():7.2f}g, \"\n",
        "          f\"max={all_predictions[:, i].max():7.2f}g, \"\n",
        "          f\"mean={all_predictions[:, i].mean():7.2f}g\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating submission file...\n",
            "\n",
            "Submission DataFrame:\n",
            "                    sample_id     target\n",
            "0   ID1001187975__Dry_Green_g  15.715257\n",
            "1    ID1001187975__Dry_Dead_g  19.350996\n",
            "2  ID1001187975__Dry_Clover_g   1.302833\n",
            "3         ID1001187975__GDM_g  19.119936\n",
            "4   ID1001187975__Dry_Total_g  34.255455\n",
            "\n",
            "Shape: (5, 2)\n",
            "Expected: (5, 2)\n",
            "\n",
            "Quality checks:\n",
            "  NaN values: 0 \u2713\n",
            "  Infinite values: 0 \u2713\n",
            "  Negative values: 0 \u2713\n",
            "  Correct columns: True \u2713\n",
            "\n",
            "\u2705 File verified: submission.csv (191 bytes)\n",
            "\n",
            "================================================================================\n",
            "\u2705 SUBMISSION FILE CREATED: submission.csv\n",
            "================================================================================\n",
            "\n",
            "File details:\n",
            "  Filename: submission.csv (required by Kaggle)\n",
            "  Location: /Users/tim/Code/Tim/csiro-biomass/submission.csv\n",
            "  Rows: 5\n",
            "  Images: 1\n",
            "  Format: Long format (sample_id, target)\n",
            "\n",
            "Model info:\n",
            "  Model: Simple ResNet18 Baseline (Model 1)\n",
            "  Validation R\u00b2: +0.6352 (10 epochs)\n",
            "  Training: ResNet18 + basic augmentation, no ColorJitter\n",
            "\n",
            "Expected Kaggle score: 0.55-0.60\n",
            "  (Better than Model 4b's 0.51 due to less overfitting)\n",
            "\n",
            "Next steps:\n",
            "  1. Download submission.csv from notebook output\n",
            "  2. Submit to Kaggle competition\n",
            "  3. Compare with Model 4b score (0.51)\n",
            "  4. If Model 1 > 0.51: confirms overfitting hypothesis\n",
            "  5. If Model 1 < 0.51: indicates distribution shift problem\n",
            "\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# Cell 8: Create Submission File\n",
        "\n",
        "import os\n",
        "\n",
        "print(\"Creating submission file...\\n\")\n",
        "\n",
        "# Convert predictions to long format (one row per sample_id)\n",
        "submission_rows = []\n",
        "\n",
        "for idx, img_path in enumerate(unique_images_df['image_path'].tolist()):\n",
        "    # Extract image ID from path (e.g., 'test/ID1001187975.jpg' -> 'ID1001187975')\n",
        "    image_id = Path(img_path).stem  # Get filename without extension\n",
        "    \n",
        "    # Create one row per target (5 rows per image)\n",
        "    for target_idx, target_name in enumerate(TARGET_COLS):\n",
        "        sample_id = f\"{image_id}__{target_name}\"  # Format: ImageID__TargetName\n",
        "        target_value = all_predictions[idx, target_idx]\n",
        "        \n",
        "        submission_rows.append({\n",
        "            'sample_id': sample_id,\n",
        "            'target': target_value\n",
        "        })\n",
        "\n",
        "# Create DataFrame\n",
        "submission = pd.DataFrame(submission_rows)\n",
        "\n",
        "print(\"Submission DataFrame:\")\n",
        "print(submission.head(10))\n",
        "print(f\"\\nShape: {submission.shape}\")\n",
        "print(f\"Expected: ({len(unique_images_df) * 5}, 2)\")\n",
        "\n",
        "# Quality checks\n",
        "print(f\"\\nQuality checks:\")\n",
        "print(f\"  NaN values: {submission.isna().sum().sum()} \u2713\" if submission.isna().sum().sum() == 0 else f\"  \u26a0\ufe0f  NaN values: {submission.isna().sum().sum()}\")\n",
        "print(f\"  Infinite values: {np.isinf(submission['target']).sum()} \u2713\" if np.isinf(submission['target']).sum() == 0 else f\"  \u26a0\ufe0f  Infinite values: {np.isinf(submission['target']).sum()}\")\n",
        "print(f\"  Negative values: {(submission['target'] < 0).sum()} \u2713\" if (submission['target'] < 0).sum() == 0 else f\"  \u26a0\ufe0f  Negative values: {(submission['target'] < 0).sum()}\")\n",
        "print(f\"  Correct columns: {list(submission.columns) == ['sample_id', 'target']} \u2713\" if list(submission.columns) == ['sample_id', 'target'] else f\"  \u26a0\ufe0f  Columns: {list(submission.columns)}\")\n",
        "\n",
        "# IMPORTANT: Save to current working directory for Kaggle compatibility\n",
        "output_path = 'submission.csv'\n",
        "submission.to_csv(output_path, index=False)\n",
        "\n",
        "# Verify file was created\n",
        "if os.path.exists(output_path):\n",
        "    file_size = os.path.getsize(output_path)\n",
        "    print(f\"\\n\u2705 File verified: {output_path} ({file_size:,} bytes)\")\n",
        "else:\n",
        "    raise FileNotFoundError(f\"Failed to create {output_path}\")\n",
        "\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(\"\u2705 SUBMISSION FILE CREATED: submission.csv\")\n",
        "print(f\"{'='*80}\")\n",
        "print(f\"\\nFile details:\")\n",
        "print(f\"  Filename: submission.csv (required by Kaggle)\")\n",
        "print(f\"  Location: {os.path.abspath(output_path)}\")\n",
        "print(f\"  Rows: {len(submission):,}\")\n",
        "print(f\"  Images: {len(unique_images_df)}\")\n",
        "print(f\"  Format: Long format (sample_id, target)\")\n",
        "print(f\"\\nModel info:\")\n",
        "print(f\"  Model: Simple ResNet18 Baseline (Model 1)\")\n",
        "print(f\"  Validation R\u00b2: +0.6352 (10 epochs)\")\n",
        "print(f\"  Training: ResNet18 + basic augmentation, no ColorJitter\")\n",
        "print(f\"\\nExpected Kaggle score: 0.55-0.60\")\n",
        "print(f\"  (Better than Model 4b's 0.51 due to less overfitting)\")\n",
        "print(f\"\\nNext steps:\")\n",
        "print(f\"  1. Download submission.csv from notebook output\")\n",
        "print(f\"  2. Submit to Kaggle competition\")\n",
        "print(f\"  3. Compare with Model 4b score (0.51)\")\n",
        "print(f\"  4. If Model 1 > 0.51: confirms overfitting hypothesis\")\n",
        "print(f\"  5. If Model 1 < 0.51: indicates distribution shift problem\")\n",
        "print(f\"\\n{'='*80}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 9: Final Verification (For Kaggle)\n",
        "\n",
        "import os\n",
        "import glob\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"FINAL VERIFICATION\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# List all CSV files in current directory\n",
        "csv_files = glob.glob('*.csv')\n",
        "print(f\"\\nCSV files in current directory:\")\n",
        "for f in csv_files:\n",
        "    size = os.path.getsize(f)\n",
        "    print(f\"  {f}: {size:,} bytes\")\n",
        "\n",
        "# Verify submission.csv specifically\n",
        "if os.path.exists('submission.csv'):\n",
        "    size = os.path.getsize('submission.csv')\n",
        "    print(f\"\\n\u2705 SUCCESS! submission.csv exists ({size:,} bytes)\")\n",
        "    print(f\"   Absolute path: {os.path.abspath('submission.csv')}\")\n",
        "    \n",
        "    # Show first few lines\n",
        "    import pandas as pd\n",
        "    sub = pd.read_csv('submission.csv')\n",
        "    print(f\"\\nFirst 5 rows:\")\n",
        "    print(sub.head())\n",
        "    print(f\"\\nTotal rows: {len(sub)}\")\n",
        "    print(f\"Columns: {list(sub.columns)}\")\n",
        "else:\n",
        "    print(f\"\\n\u274c ERROR! submission.csv not found!\")\n",
        "    print(f\"Current directory: {os.getcwd()}\")\n",
        "    print(f\"Files in directory: {os.listdir('.')}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}